# %% [markdown]
# # ğŸ“˜ M3.4 Pandas é€²éšæ•¸æ“šåˆä½µèˆ‡é€£æ¥æŠ€è¡“
# 
# æœ¬æ•™å­¸å°‡æ·±å…¥æ¢è¨ Pandas ä¸­æ•¸æ“šåˆä½µ(Merge)èˆ‡é€£æ¥(Join)çš„é€²éšæŠ€è¡“ï¼Œè¶…è¶ŠåŸºæœ¬æ“ä½œä»¥æ‡‰å°è¤‡é›œçš„æ•¸æ“šæ•´åˆå ´æ™¯ã€‚
# æˆ‘å€‘å°‡å­¸ç¿’å¦‚ä½•è™•ç†è¤‡é›œçš„æ•¸æ“šé—œä¿‚ã€å„ªåŒ–å¤§å‹æ•¸æ“šé›†çš„åˆä½µæ“ä½œï¼Œä»¥åŠæ‡‰ç”¨é€™äº›æŠ€è¡“è§£æ±ºå¯¦éš›æ¥­å‹™å•é¡Œã€‚

# %% [markdown]
# ## ğŸ¯ æ•™å­¸ç›®æ¨™
# 
# - ğŸ” æŒæ¡è¤‡é›œæ•¸æ“šåˆä½µç­–ç•¥èˆ‡å¤šç¨®é€£æ¥æ–¹å¼çš„æ·±å…¥æ‡‰ç”¨
# - ğŸ”„ å­¸ç¿’é©—è­‰èˆ‡è¨ºæ–·åˆä½µçµæœçš„æŠ€è¡“ï¼Œç¢ºä¿æ•¸æ“šå®Œæ•´æ€§
# - ğŸ“Š ç†è§£å¤§æ•¸æ“šé›†åˆä½µçš„æ€§èƒ½å„ªåŒ–æ–¹æ³•
# - ğŸ§® æ¢ç´¢é€²éšåˆä½µæ–¹å¼ï¼Œå¦‚æ¢ä»¶åˆä½µã€éšå±¤åŒ–æ•¸æ“šé€£æ¥ç­‰
# - ğŸ› ï¸ é‹ç”¨åˆä½µæŠ€è¡“è§£æ±ºè¤‡é›œæ¥­å‹™å ´æ™¯çš„æ•¸æ“šæ•´åˆå•é¡Œ

# %% [markdown]
# ## ğŸ§° 1. ç’°å¢ƒè¨­ç½®

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
from functools import wraps

# è¨­ç½®é¡¯ç¤ºé¸é …
pd.set_option('display.max_rows', 15)
pd.set_option('display.max_columns', 12)
pd.set_option('display.width', 100)
pd.set_option('display.precision', 2)

# è¨ˆæ™‚è£é£¾å™¨ç”¨æ–¼æ€§èƒ½æ¸¬è©¦
def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} åŸ·è¡Œæ™‚é–“: {(end_time - start_time):.6f} ç§’")
        return result
    return wrapper

# %% [markdown]
# ## ğŸ“Š 2. å‰µå»ºè¤‡é›œçš„ç¤ºä¾‹æ•¸æ“šé›†

# %%
# å‰µå»ºæ›´è¤‡é›œçš„éŠ·å”®æ•¸æ“šç’°å¢ƒ
np.random.seed(42)

# 1. å®¢æˆ¶æ•¸æ“š
customers = pd.DataFrame({
    'customer_id': range(1, 101),
    'customer_name': [f'Customer_{i}' for i in range(1, 101)],
    'customer_segment': np.random.choice(['Premium', 'Standard', 'Basic'], 100),
    'signup_date': pd.date_range(start='2020-01-01', periods=100),
    'region_id': np.random.randint(1, 6, 100)
})

# 2. å€åŸŸæ•¸æ“š
regions = pd.DataFrame({
    'region_id': range(1, 6),
    'region_name': ['North', 'South', 'East', 'West', 'Central'],
    'country': ['USA', 'Canada', 'UK', 'Germany', 'France'],
    'regional_manager': ['John Smith', 'Emily Johnson', 'David Brown', 'Sarah Wilson', 'Michael Lee']
})

# 3. ç”¢å“æ•¸æ“š
products = pd.DataFrame({
    'product_id': range(101, 121),
    'product_name': [f'Product_{i}' for i in range(101, 121)],
    'category_id': np.random.randint(1, 5, 20),
    'unit_price': np.random.uniform(10, 100, 20).round(2),
    'supplier_id': np.random.randint(1, 11, 20)
})

# 4. é¡åˆ¥æ•¸æ“š
categories = pd.DataFrame({
    'category_id': range(1, 5),
    'category_name': ['Electronics', 'Clothing', 'Home Goods', 'Food & Beverage'],
    'category_manager': ['Alice Johnson', 'Bob Williams', 'Carol Davis', 'Dan Miller']
})

# 5. ä¾›æ‡‰å•†æ•¸æ“š
suppliers = pd.DataFrame({
    'supplier_id': range(1, 11),
    'supplier_name': [f'Supplier_{i}' for i in range(1, 11)],
    'supplier_country': np.random.choice(['USA', 'China', 'India', 'Germany', 'Japan'], 10),
    'lead_time_days': np.random.randint(5, 30, 10)
})

# 6. è¨‚å–®æ•¸æ“š (å°‡å‰µå»º1000ç­†è¨‚å–®)
order_headers = pd.DataFrame({
    'order_id': range(1001, 2001),
    'customer_id': np.random.choice(customers['customer_id'], 1000),
    'order_date': pd.date_range(start='2023-01-01', periods=1000),
    'status': np.random.choice(['Completed', 'Shipped', 'Processing', 'Cancelled'], 1000, 
                              p=[0.7, 0.15, 0.1, 0.05]),
    'sales_rep_id': np.random.randint(1, 21, 1000)
})

# 7. è¨‚å–®è©³æƒ… (æ¯å€‹è¨‚å–®å¹³å‡2-5å€‹ç”¢å“é …ç›®ï¼Œå…±ç´„3000å€‹è¨‚å–®é …ç›®)
num_order_items = 3000
order_items = pd.DataFrame({
    'order_item_id': range(1, num_order_items + 1),
    'order_id': np.random.choice(order_headers['order_id'], num_order_items),
    'product_id': np.random.choice(products['product_id'], num_order_items),
    'quantity': np.random.randint(1, 10, num_order_items),
    'discount': np.random.choice([0, 0.05, 0.1, 0.15, 0.2], num_order_items)
})

# 8. éŠ·å”®ä»£è¡¨æ•¸æ“š
sales_reps = pd.DataFrame({
    'sales_rep_id': range(1, 21),
    'sales_rep_name': [f'Rep_{i}' for i in range(1, 21)],
    'region_id': np.random.randint(1, 6, 20),
    'hire_date': pd.date_range(start='2018-01-01', periods=20)
})

# 9. æ”¯ä»˜æ•¸æ“š (åªæœ‰å·²å®Œæˆçš„è¨‚å–®æœ‰æ”¯ä»˜è¨˜éŒ„)
completed_orders = order_headers[order_headers['status'] == 'Completed']['order_id']
payments = pd.DataFrame({
    'payment_id': range(1, len(completed_orders) + 1),
    'order_id': completed_orders.values,
    'payment_date': pd.date_range(start='2023-01-05', periods=len(completed_orders)),
    'payment_method': np.random.choice(['Credit Card', 'PayPal', 'Bank Transfer', 'Cash'], len(completed_orders)),
    'amount': np.random.uniform(50, 500, len(completed_orders)).round(2)
})

print("å‰µå»ºäº†ä»¥ä¸‹æ•¸æ“šè¡¨:")
print(f"1. å®¢æˆ¶ (Customers): {customers.shape[0]} è¡Œ x {customers.shape[1]} åˆ—")
print(f"2. å€åŸŸ (Regions): {regions.shape[0]} è¡Œ x {regions.shape[1]} åˆ—")
print(f"3. ç”¢å“ (Products): {products.shape[0]} è¡Œ x {products.shape[1]} åˆ—")
print(f"4. é¡åˆ¥ (Categories): {categories.shape[0]} è¡Œ x {categories.shape[1]} åˆ—")
print(f"5. ä¾›æ‡‰å•† (Suppliers): {suppliers.shape[0]} è¡Œ x {suppliers.shape[1]} åˆ—")
print(f"6. è¨‚å–®æ¨™é ­ (Order Headers): {order_headers.shape[0]} è¡Œ x {order_headers.shape[1]} åˆ—")
print(f"7. è¨‚å–®é …ç›® (Order Items): {order_items.shape[0]} è¡Œ x {order_items.shape[1]} åˆ—")
print(f"8. éŠ·å”®ä»£è¡¨ (Sales Reps): {sales_reps.shape[0]} è¡Œ x {sales_reps.shape[1]} åˆ—")
print(f"9. æ”¯ä»˜ (Payments): {payments.shape[0]} è¡Œ x {payments.shape[1]} åˆ—")

# æŸ¥çœ‹è¨‚å–®æ¨™é ­èˆ‡é …ç›®æ•¸æ“šç¤ºä¾‹
print("\nè¨‚å–®æ¨™é ­ç¤ºä¾‹:")
print(order_headers.head(3))
print("\nè¨‚å–®é …ç›®ç¤ºä¾‹:")
print(order_items.head(3))

# %% [markdown]
# ## ğŸ“Š 3. è¤‡é›œæ•¸æ“šé—œä¿‚è™•ç†

# %% [markdown]
# ### 3.1 å¤šè¡¨é—œè¯åˆ†æ

# %%
# å»ºç«‹è¨‚å–®çš„å®Œæ•´è¦–åœ–ï¼ˆè¨‚å–®ã€å®¢æˆ¶ã€ç”¢å“ã€é¡åˆ¥ã€éŠ·å”®ä»£è¡¨ï¼‰
@timing_decorator
def create_order_full_view():
    # ç¬¬ä¸€æ­¥ï¼šåˆä½µè¨‚å–®èˆ‡è¨‚å–®é …ç›®
    order_with_items = pd.merge(
        order_headers,
        order_items,
        on='order_id',
        how='inner'
    )
    
    # ç¬¬äºŒæ­¥ï¼šåŠ å…¥ç”¢å“ä¿¡æ¯
    order_with_products = pd.merge(
        order_with_items,
        products,
        on='product_id',
        how='inner'
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šåŠ å…¥é¡åˆ¥ä¿¡æ¯
    order_with_categories = pd.merge(
        order_with_products,
        categories,
        on='category_id',
        how='inner'
    )
    
    # ç¬¬å››æ­¥ï¼šåŠ å…¥å®¢æˆ¶ä¿¡æ¯
    order_with_customers = pd.merge(
        order_with_categories,
        customers,
        on='customer_id',
        how='inner'
    )
    
    # ç¬¬äº”æ­¥ï¼šåŠ å…¥éŠ·å”®ä»£è¡¨ä¿¡æ¯
    full_order_view = pd.merge(
        order_with_customers,
        sales_reps,
        on='sales_rep_id',
        how='inner'
    )
    
    # è¨ˆç®—è¨‚å–®é …ç›®é‡‘é¡
    full_order_view['item_price'] = full_order_view['unit_price'] * full_order_view['quantity'] * (1 - full_order_view['discount'])
    
    return full_order_view

# å‰µå»ºå®Œæ•´è¨‚å–®è¦–åœ–
full_orders = create_order_full_view()

print(f"å®Œæ•´è¨‚å–®è¦–åœ–ç¶­åº¦: {full_orders.shape}")
print("åŒ…å«çš„åˆ—:")
print(full_orders.columns.tolist())
print("\nå‰3ç­†æ•¸æ“š:")
print(full_orders.head(3))

# %% [markdown]
# ### 3.2 ç†è§£åˆä½µå¾Œæ•¸æ“šé‡è®ŠåŒ–

# %%
# åˆ†æé€æ­¥åˆä½µéç¨‹ä¸­çš„æ•¸æ“šé‡è®ŠåŒ–
def analyze_merge_cardinality():
    # è¨˜éŒ„æ¯ä¸€æ­¥åˆä½µçš„æ•¸æ“šé‡
    steps = []
    
    # åŸå§‹è¨‚å–®é …ç›®æ•¸é‡
    steps.append(("åŸå§‹è¨‚å–®é …ç›®", len(order_items)))
    
    # æ­¥é©Ÿ 1: è¨‚å–®æ¨™é ­ + è¨‚å–®é …ç›®
    step1 = pd.merge(order_items, order_headers, on='order_id')
    steps.append(("è¨‚å–®+è¨‚å–®é …ç›®", len(step1)))
    
    # æ­¥é©Ÿ 2: + ç”¢å“
    step2 = pd.merge(step1, products, on='product_id')
    steps.append(("+ ç”¢å“", len(step2)))
    
    # æ­¥é©Ÿ 3: + é¡åˆ¥
    step3 = pd.merge(step2, categories, on='category_id')
    steps.append(("+ é¡åˆ¥", len(step3)))
    
    # æ­¥é©Ÿ 4: + å®¢æˆ¶
    step4 = pd.merge(step3, customers, on='customer_id')
    steps.append(("+ å®¢æˆ¶", len(step4)))
    
    # æ­¥é©Ÿ 5: + éŠ·å”®ä»£è¡¨
    step5 = pd.merge(step4, sales_reps, on='sales_rep_id')
    steps.append(("+ éŠ·å”®ä»£è¡¨", len(step5)))
    
    # å‰µå»ºçµæœ DataFrame
    cardinality_df = pd.DataFrame(steps, columns=["åˆä½µæ­¥é©Ÿ", "è¨˜éŒ„æ•¸é‡"])
    cardinality_df["è®ŠåŒ–ç‡"] = cardinality_df["è¨˜éŒ„æ•¸é‡"].pct_change().fillna(0) * 100
    cardinality_df["è®ŠåŒ–ç‡"] = cardinality_df["è®ŠåŒ–ç‡"].map(lambda x: f"{x:.2f}%" if x != 0 else "åŸºæº–")
    cardinality_df["èªªæ˜"] = [
        "åŸºæº–æ•¸é‡",
        "1:1 é—œä¿‚ï¼Œæ¯å€‹è¨‚å–®é …ç›®å°æ‡‰ä¸€å€‹è¨‚å–®",
        "1:1 é—œä¿‚ï¼Œæ¯å€‹è¨‚å–®é …ç›®å°æ‡‰ä¸€å€‹ç”¢å“",
        "1:1 é—œä¿‚ï¼Œæ¯å€‹ç”¢å“å°æ‡‰ä¸€å€‹é¡åˆ¥",
        "1:1 é—œä¿‚ï¼Œæ¯å€‹è¨‚å–®å°æ‡‰ä¸€å€‹å®¢æˆ¶",
        "1:1 é—œä¿‚ï¼Œæ¯å€‹è¨‚å–®å°æ‡‰ä¸€å€‹éŠ·å”®ä»£è¡¨"
    ]
    
    return cardinality_df

cardinality_analysis = analyze_merge_cardinality()
print("åˆä½µéç¨‹ä¸­çš„æ•¸æ“šé‡è®ŠåŒ–åˆ†æ:")
print(cardinality_analysis)

# å¯è¦–åŒ–æ•¸æ“šé‡è®ŠåŒ–
plt.figure(figsize=(12, 6))
plt.bar(cardinality_analysis["åˆä½µæ­¥é©Ÿ"], cardinality_analysis["è¨˜éŒ„æ•¸é‡"])
plt.title("åˆä½µéç¨‹ä¸­çš„æ•¸æ“šé‡è®ŠåŒ–")
plt.ylabel("è¨˜éŒ„æ•¸é‡")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 3.3 é¿å…ç¬›å¡çˆ¾ç©å•é¡Œ

# %%
# ç¤ºç¯„ç¬›å¡çˆ¾ç©å•é¡Œ
def demonstrate_cartesian_problem():
    # å‰µå»ºç°¡å–®ç¤ºä¾‹æ•¸æ“š
    df1 = pd.DataFrame({
        'key': [1, 2, 3],
        'value_a': ['a1', 'a2', 'a3']
    })
    
    df2 = pd.DataFrame({
        'key': [1, 1, 1, 2, 2, 3],  # æ³¨æ„é€™è£¡æœ‰é‡è¤‡çš„éµ
        'value_b': ['b1', 'b2', 'b3', 'b4', 'b5', 'b6']
    })
    
    print("DataFrame 1:")
    print(df1)
    print("\nDataFrame 2 (åŒ…å«é‡è¤‡çš„éµ):")
    print(df2)
    
    # åŸ·è¡Œå…§é€£æ¥
    merged = pd.merge(df1, df2, on='key')
    print("\nåˆä½µçµæœ (ç”¢ç”Ÿç¬›å¡çˆ¾ç©):")
    print(merged)
    print(f"åˆä½µå‰: df1æœ‰{len(df1)}è¡Œ, df2æœ‰{len(df2)}è¡Œ")
    print(f"åˆä½µå¾Œ: {len(merged)}è¡Œ")
    
    # åˆ†æé€£æ¥åŸºæ•¸
    key_counts_df1 = df1['key'].value_counts().reset_index()
    key_counts_df1.columns = ['key', 'count_df1']
    
    key_counts_df2 = df2['key'].value_counts().reset_index()
    key_counts_df2.columns = ['key', 'count_df2']
    
    key_analysis = pd.merge(key_counts_df1, key_counts_df2, on='key', how='outer').fillna(0)
    key_analysis['expected_rows'] = key_analysis['count_df1'] * key_analysis['count_df2']
    
    print("\néµåˆ†ä½ˆåˆ†æ:")
    print(key_analysis)
    print(f"é æœŸçš„åˆä½µè¡Œæ•¸: {key_analysis['expected_rows'].sum()}")
    
    return df1, df2, merged

# åŸ·è¡Œæ¼”ç¤º
df1, df2, cartesian_result = demonstrate_cartesian_problem()

# æä¾›é¿å…ç¬›å¡çˆ¾ç©çš„æ–¹æ³•
print("\né¿å…ç¬›å¡çˆ¾ç©çš„æ–¹æ³•:")
print("1. åœ¨åˆä½µå‰æª¢æŸ¥ä¸¦åˆªé™¤é‡è¤‡çš„éµå€¼:")
df2_unique = df2.drop_duplicates(subset=['key'])
safe_merge = pd.merge(df1, df2_unique, on='key')
print(f"   å»é™¤é‡è¤‡å¾Œåˆä½µ: {len(safe_merge)}è¡Œ")

print("2. ä½¿ç”¨ merge å‰çš„é©—è­‰å‡½æ•¸:")
def validate_merge_keys(df1, df2, key):
    df1_key_counts = df1[key].value_counts()
    df2_key_counts = df2[key].value_counts()
    
    potential_explosion = 0
    for k in df1_key_counts.index:
        if k in df2_key_counts.index:
            potential_explosion += df1_key_counts[k] * df2_key_counts[k]
    
    print(f"   åˆä½µå‰: df1æœ‰{len(df1)}è¡Œ, df2æœ‰{len(df2)}è¡Œ")
    print(f"   æ½›åœ¨åˆä½µçµæœ: {potential_explosion}è¡Œ")
    
    explosion_factor = potential_explosion / (len(df1) + len(df2))
    print(f"   çˆ†ç‚¸å› å­: {explosion_factor:.2f}x")
    
    if explosion_factor > 2:
        print("   è­¦å‘Š: å¯èƒ½ç™¼ç”Ÿå¤§é‡æ•¸æ“šè†¨è„¹!")
        return False
    return True

print("\né©—è­‰åˆä½µéµ:")
is_safe = validate_merge_keys(df1, df2, 'key')

# %% [markdown]
# ## ğŸ“Š 4. åˆä½µé©—è­‰èˆ‡è¨ºæ–·æŠ€è¡“

# %% [markdown]
# ### 4.1 é©—è­‰åˆä½µæ“ä½œçš„å®Œæ•´æ€§

# %%
# æª¢æŸ¥åˆä½µæ˜¯å¦éºæ¼æ•¸æ“š
def check_merge_completeness(left_df, right_df, merged_df, left_on, right_on=None, how='inner'):
    if right_on is None:
        right_on = left_on
    
    # è¨ˆç®—é æœŸçš„çµæœå¤§å°
    if how == 'inner':
        left_keys = set(left_df[left_on])
        right_keys = set(right_df[right_on])
        common_keys = left_keys.intersection(right_keys)
        
        left_matched = left_df[left_df[left_on].isin(common_keys)]
        right_matched = right_df[right_df[right_on].isin(common_keys)]
        
        expected_rows = len(left_matched) if len(left_matched) <= len(right_matched) else len(right_matched)
        if len(left_matched) > 0 and len(right_matched) > 0:
            # å°æ–¼å¤šå°å¤šé—œä¿‚ï¼Œè¨ˆç®—ä¸Šé™
            left_counts = left_matched[left_on].value_counts()
            right_counts = right_matched[right_on].value_counts()
            max_rows = sum(left_counts[k] * right_counts[k] for k in common_keys if k in left_counts and k in right_counts)
            expected_rows = max_rows
    
    elif how == 'left':
        expected_rows = len(left_df)
    elif how == 'right':
        expected_rows = len(right_df)
    elif how == 'outer':
        # å¤–é€£æ¥å¯èƒ½æ¯”è¼ƒè¤‡é›œï¼Œé€™è£¡ç°¡åŒ–è™•ç†
        expected_rows = len(left_df) + len(right_df) - len(set(left_df[left_on]).intersection(set(right_df[right_on])))
    
    actual_rows = len(merged_df)
    
    print(f"åˆä½µå®Œæ•´æ€§æª¢æŸ¥ ({how} join):")
    print(f"å·¦è¡¨è¡Œæ•¸: {len(left_df)}")
    print(f"å³è¡¨è¡Œæ•¸: {len(right_df)}")
    print(f"åˆä½µçµæœè¡Œæ•¸: {actual_rows}")
    print(f"é æœŸçµæœè¡Œæ•¸ (ç´„): {expected_rows}")
    
    if how in ['left', 'right', 'outer']:
        # æª¢æŸ¥ç©ºå€¼æƒ…æ³
        if how == 'left' or how == 'outer':
            null_count = merged_df[right_df.columns[0]].isna().sum()
            print(f"å³è¡¨åˆ—ä¸­çš„ç©ºå€¼æ•¸: {null_count} ({null_count/len(merged_df):.2%})")
        
        if how == 'right' or how == 'outer':
            null_count = merged_df[left_df.columns[0]].isna().sum()
            print(f"å·¦è¡¨åˆ—ä¸­çš„ç©ºå€¼æ•¸: {null_count} ({null_count/len(merged_df):.2%})")
    
    return {
        'left_rows': len(left_df),
        'right_rows': len(right_df),
        'merged_rows': actual_rows,
        'expected_rows': expected_rows,
        'match_rate': actual_rows / expected_rows if expected_rows > 0 else 0
    }

# ç¤ºç¯„åˆä½µé©—è­‰
orders_sample = order_headers.sample(100, random_state=42)
customers_sample = customers.sample(50, random_state=42)

# åŸ·è¡Œå·¦é€£æ¥
left_join = pd.merge(orders_sample, customers_sample, on='customer_id', how='left')

# é©—è­‰åˆä½µçµæœ
merge_stats = check_merge_completeness(orders_sample, customers_sample, left_join, 'customer_id', how='left')

# %% [markdown]
# ### 4.2 åˆä½µå¾Œæ•¸æ“šä¸€è‡´æ€§é©—è­‰

# %%
# åˆä½µå¾Œçš„æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
def validate_merge_consistency(original_df1, original_df2, merged_df, key):
    """æª¢æŸ¥åˆä½µå¾Œçš„æ•¸æ“šæ˜¯å¦èˆ‡åŸå§‹æ•¸æ“šé›†ä¸€è‡´"""
    # æª¢æŸ¥éµçš„å”¯ä¸€æ€§
    print("æª¢æŸ¥åˆä½µéµçš„å”¯ä¸€æ€§:")
    print(f"å·¦è¡¨ '{key}' å”¯ä¸€å€¼æ•¸é‡: {original_df1[key].nunique()}")
    print(f"å³è¡¨ '{key}' å”¯ä¸€å€¼æ•¸é‡: {original_df2[key].nunique()}")
    print(f"åˆä½µè¡¨ '{key}' å”¯ä¸€å€¼æ•¸é‡: {merged_df[key].nunique()}")
    
    # æª¢æŸ¥èšåˆæŒ‡æ¨™æ˜¯å¦ä¿æŒä¸€è‡´
    print("\næª¢æŸ¥æ•¸å€¼åˆ—çš„èšåˆæŒ‡æ¨™:")
    
    # é¸æ“‡æ•¸å€¼åˆ—
    numeric_cols1 = original_df1.select_dtypes(include=['number']).columns
    numeric_cols2 = original_df2.select_dtypes(include=['number']).columns
    
    # æ’é™¤åˆä½µéµ
    numeric_cols1 = [col for col in numeric_cols1 if col != key]
    numeric_cols2 = [col for col in numeric_cols2 if col != key]
    
    # æª¢æŸ¥å·¦è¡¨çš„æ•¸å€¼åˆ—
    for col in numeric_cols1[:2]:  # åƒ…æª¢æŸ¥å‰å…©åˆ—ä½œç‚ºç¤ºä¾‹
        if col in merged_df.columns:
            orig_sum = original_df1[col].sum()
            merged_sum = merged_df[col].sum()
            
            print(f"åˆ— '{col}' ç¸½å’Œ - åŸå§‹: {orig_sum}, åˆä½µå¾Œ: {merged_sum}, å·®ç•°: {abs(orig_sum - merged_sum)}")
    
    # æª¢æŸ¥å³è¡¨çš„æ•¸å€¼åˆ—
    for col in numeric_cols2[:2]:  # åƒ…æª¢æŸ¥å‰å…©åˆ—ä½œç‚ºç¤ºä¾‹
        if col in merged_df.columns:
            orig_sum = original_df2[col].sum()
            merged_sum = merged_df[col].sum()
            
            print(f"åˆ— '{col}' ç¸½å’Œ - åŸå§‹: {orig_sum}, åˆä½µå¾Œ: {merged_sum}, å·®ç•°: {abs(orig_sum - merged_sum)}")
    
    # æª¢æŸ¥æ¯å€‹åˆä½µéµå€¼çš„è¡Œæ•¸ä¸€è‡´æ€§
    print("\næª¢æŸ¥å„éµå€¼çš„è¡Œæ•¸:")
    key_counts_orig1 = original_df1[key].value_counts().sort_index()
    key_counts_orig2 = original_df2[key].value_counts().sort_index()
    key_counts_merged = merged_df[key].value_counts().sort_index()
    
    # å–æ¨£å±•ç¤ºå¹¾å€‹éµå€¼
    sample_keys = sorted(list(set(key_counts_orig1.index) & set(key_counts_orig2.index)))[:3]
    
    for k in sample_keys:
        count1 = key_counts_orig1.get(k, 0)
        count2 = key_counts_orig2.get(k, 0)
        count_merged = key_counts_merged.get(k, 0)
        
        print(f"éµå€¼ {k}: å·¦è¡¨ {count1} è¡Œ, å³è¡¨ {count2} è¡Œ, åˆä½µè¡¨ {count_merged} è¡Œ")
        
        # å¦‚æœæ˜¯ä¸€å°å¤šæˆ–å¤šå°å¤šé—œä¿‚ï¼Œè§£é‡‹é æœŸè¡Œæ•¸
        if count1 > 1 or count2 > 1:
            expected = count1 * count2
            print(f"  é æœŸè¡Œæ•¸: {count1} x {count2} = {expected}")
    
    return True

# ç¤ºç¯„é©—è­‰
validate_merge_consistency(orders_sample, customers, left_join, 'customer_id')

# %% [markdown]
# ## ğŸ“Š 5. é«˜æ€§èƒ½åˆä½µæŠ€è¡“

# %% [markdown]
# ### 5.1 å¤§æ•¸æ“šé›†åˆä½µå„ªåŒ–

# %%
# å‰µå»ºè¼ƒå¤§çš„æ¸¬è©¦æ•¸æ“šé›†
def create_large_test_data(size=1000000):
    """å‰µå»ºå¤§å‹æ¸¬è©¦æ•¸æ“šé›†ç”¨æ–¼æ€§èƒ½æ¸¬è©¦"""
    np.random.seed(42)
    large_df1 = pd.DataFrame({
        'id': np.arange(size),
        'key': np.random.randint(0, size // 10, size),
        'value_a': np.random.randn(size)
    })
    
    large_df2 = pd.DataFrame({
        'key': np.random.randint(0, size // 10, size // 5),
        'value_b': np.random.randn(size // 5)
    })
    
    return large_df1, large_df2

# è¼ƒå°è¦æ¨¡æ¸¬è©¦ (ç‚ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨è¼ƒå°çš„æ•¸æ“šé›†)
large_df1, large_df2 = create_large_test_data(size=100000)

print(f"å¤§å‹æ•¸æ“šé›† 1: {large_df1.shape}")
print(f"å¤§å‹æ•¸æ“šé›† 2: {large_df2.shape}")

# æ€§èƒ½æ¸¬è©¦ï¼šæœªå„ªåŒ–çš„åˆä½µ
@timing_decorator
def merge_unoptimized(df1, df2):
    return pd.merge(df1, df2, on='key')

# æ€§èƒ½æ¸¬è©¦ï¼šé æ’åºå„ªåŒ–
@timing_decorator
def merge_presorted(df1, df2):
    df1_sorted = df1.sort_values('key')
    df2_sorted = df2.sort_values('key')
    return pd.merge(df1_sorted, df2_sorted, on='key', sort=False)

# æ€§èƒ½æ¸¬è©¦ï¼šé¡åˆ¥å„ªåŒ–
@timing_decorator
def merge_category(df1, df2):
    df1_cat = df1.copy()
    df2_cat = df2.copy()
    df1_cat['key'] = df1_cat['key'].astype('category')
    df2_cat['key'] = df2_cat['key'].astype('category')
    return pd.merge(df1_cat, df2_cat, on='key')

# æ€§èƒ½æ¸¬è©¦ï¼šåˆ†å¡Šè™•ç†
@timing_decorator
def merge_chunked(df1, df2, chunk_size=20000):
    result_chunks = []
    
    # æ ¹æ“š chunk_size åˆ†å‰²ç¬¬ä¸€å€‹ DataFrame
    for i in range(0, len(df1), chunk_size):
        chunk = df1.iloc[i:i+chunk_size]
        # åˆä½µæ¯å€‹å¡Šèˆ‡ç¬¬äºŒå€‹ DataFrame
        merged_chunk = pd.merge(chunk, df2, on='key')
        result_chunks.append(merged_chunk)
    
    # å°‡æ‰€æœ‰å¡Šé€£æ¥æˆæœ€çµ‚çµæœ
    return pd.concat(result_chunks, ignore_index=True)

# é‹è¡Œæ€§èƒ½æ¸¬è©¦
print("\næ€§èƒ½æ¸¬è©¦çµæœ:")
result1 = merge_unoptimized(large_df1, large_df2)
result2 = merge_presorted(large_df1, large_df2)
result3 = merge_category(large_df1, large_df2)
result4 = merge_chunked(large_df1, large_df2)

print(f"\nåˆä½µçµæœè¡Œæ•¸ (æª¢æŸ¥çµæœä¸€è‡´æ€§):")
print(f"æœªå„ªåŒ–: {len(result1)}")
print(f"é æ’åº: {len(result2)}")
print(f"é¡åˆ¥åŒ–: {len(result3)}")
print(f"åˆ†å¡Šè™•ç†: {len(result4)}")

# %% [markdown]
# ### 5.2 ä½¿ç”¨ SQL é€²è¡Œåˆä½µ (è¶…å¤§æ•¸æ“šé›†)

# %%
# ä½¿ç”¨ SQLite æ¼”ç¤ºè¶…å¤§æ•¸æ“šé›†åˆä½µ
import sqlite3
from io import StringIO

@timing_decorator
def merge_with_sql(df1, df2, key):
    """ä½¿ç”¨ SQLite é€²è¡Œå¤§æ•¸æ“šé›†åˆä½µ"""
    # å‰µå»ºè‡¨æ™‚çš„å…§å­˜æ•¸æ“šåº«é€£æ¥
    conn = sqlite3.connect(':memory:')
    
    # å°‡ DataFrames å¯«å…¥ SQLite è¡¨
    df1.to_sql('table1', conn, index=False, if_exists='replace')
    df2.to_sql('table2', conn, index=False, if_exists='replace')
    
    # å‰µå»ºç´¢å¼•ä»¥åŠ é€Ÿåˆä½µ
    conn.execute(f'CREATE INDEX idx_table1_{key} ON table1({key})')
    conn.execute(f'CREATE INDEX idx_table2_{key} ON table2({key})')
    
    # ç·¨å¯«åˆä½µæŸ¥è©¢
    query = f"""
    SELECT *
    FROM table1 t1
    INNER JOIN table2 t2 ON t1.{key} = t2.{key}
    """
    
    # åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›çµæœ
    result = pd.read_sql_query(query, conn)
    
    # é—œé–‰é€£æ¥
    conn.close()
    
    return result

# é‹è¡Œ SQL åˆä½µæ¸¬è©¦
result_sql = merge_with_sql(large_df1, large_df2, 'key')
print(f"SQLåˆä½µçµæœè¡Œæ•¸: {len(result_sql)}")

# æ¯”è¼ƒåˆä½µæ–¹æ³•çš„æ€§èƒ½ï¼ˆä¸åŒè¦æ¨¡çš„æ•¸æ“šé›†ï¼‰
def compare_merge_performance():
    results = []
    
    for size in [10000, 50000, 100000]:
        df1, df2 = create_large_test_data(size)
        
        # æ¸¬é‡æœªå„ªåŒ–åˆä½µçš„æ™‚é–“
        start = time.time()
        pd.merge(df1, df2, on='key')
        time_unoptimized = time.time() - start
        
        # æ¸¬é‡é æ’åºåˆä½µçš„æ™‚é–“
        start = time.time()
        df1_sorted = df1.sort_values('key')
        df2_sorted = df2.sort_values('key')
        pd.merge(df1_sorted, df2_sorted, on='key', sort=False)
        time_presorted = time.time() - start
        
        # æ¸¬é‡ SQL åˆä½µçš„æ™‚é–“
        start = time.time()
        merge_with_sql(df1, df2, 'key')
        time_sql = time.time() - start
        
        results.append({
            'size': size,
            'unoptimized': time_unoptimized,
            'presorted': time_presorted,
            'sql': time_sql
        })
    
    return pd.DataFrame(results)

# å‰µå»ºæ€§èƒ½æ¯”è¼ƒçµæœ
performance_results = compare_merge_performance()
print("\nä¸åŒè¦æ¨¡æ•¸æ“šé›†çš„åˆä½µæ€§èƒ½æ¯”è¼ƒ:")
print(performance_results)

# å¯è¦–åŒ–æ¯”è¼ƒçµæœ
plt.figure(figsize=(12, 6))
for method in ['unoptimized', 'presorted', 'sql']:
    plt.plot(performance_results['size'], performance_results[method], marker='o', label=method)
plt.title('ä¸åŒåˆä½µæ–¹æ³•çš„æ€§èƒ½æ¯”è¼ƒ')
plt.xlabel('æ•¸æ“šé›†å¤§å°')
plt.ylabel('åŸ·è¡Œæ™‚é–“ (ç§’)')
plt.legend()
plt.grid(True)
plt.show()

# %% [markdown]
# ## ğŸ“Š 6. é€²éšç‰¹æ®Šåˆä½µæŠ€è¡“

# %% [markdown]
# ### 6.1 æ¢ä»¶å¼åˆä½µ

# %%
# æ¼”ç¤ºæ¢ä»¶å¼åˆä½µ
print("æ¢ä»¶å¼åˆä½µ (Conditional Merge)")

# å‰µå»ºåƒ¹æ ¼è¡¨ (æ ¹æ“šæ—¥æœŸå’Œç”¢å“çš„åƒ¹æ ¼è¡¨)
price_history = pd.DataFrame({
    'product_id': np.repeat(np.arange(1, 6), 3),  # 5å€‹ç”¢å“ï¼Œæ¯å€‹æœ‰3å€‹ä¸åŒçš„åƒ¹æ ¼æ­·å²
    'start_date': pd.to_datetime([
        '2023-01-01', '2023-04-01', '2023-07-01',  # ç”¢å“1
        '2023-01-01', '2023-03-01', '2023-06-01',  # ç”¢å“2
        '2023-01-01', '2023-05-01', '2023-08-01',  # ç”¢å“3
        '2023-01-01', '2023-02-01', '2023-09-01',  # ç”¢å“4
        '2023-01-01', '2023-06-01', '2023-10-01'   # ç”¢å“5
    ]),
    'end_date': pd.to_datetime([
        '2023-03-31', '2023-06-30', '2023-12-31',  # ç”¢å“1
        '2023-02-28', '2023-05-31', '2023-12-31',  # ç”¢å“2
        '2023-04-30', '2023-07-31', '2023-12-31',  # ç”¢å“3
        '2023-01-31', '2023-08-31', '2023-12-31',  # ç”¢å“4
        '2023-05-31', '2023-09-30', '2023-12-31'   # ç”¢å“5
    ]),
    'unit_price': [
        10.0, 12.0, 15.0,  # ç”¢å“1
        20.0, 22.0, 25.0,  # ç”¢å“2
        30.0, 28.0, 32.0,  # ç”¢å“3
        15.0, 18.0, 20.0,  # ç”¢å“4
        25.0, 30.0, 35.0   # ç”¢å“5
    ]
})

# å‰µå»ºè¨‚å–®è³‡æ–™ï¼ŒåŒ…å«æ—¥æœŸå’Œç”¢å“
sample_orders = pd.DataFrame({
    'order_id': np.arange(1001, 1011),
    'product_id': np.random.randint(1, 6, 10),  # éš¨æ©Ÿé¸æ“‡ç”¢å“
    'order_date': pd.to_datetime([
        '2023-02-15', '2023-03-20', '2023-04-10', '2023-05-05', '2023-06-15',
        '2023-07-20', '2023-08-10', '2023-09-05', '2023-10-15', '2023-11-20'
    ]),
    'quantity': np.random.randint(1, 10, 10)
})

print("åƒ¹æ ¼æ­·å²è¡¨:")
print(price_history.head())
print("\nè¨‚å–®è¡¨:")
print(sample_orders)

# æ¢ä»¶å¼åˆä½µï¼šæ ¹æ“šæ—¥æœŸç¯„åœåˆä½µæ­£ç¢ºçš„åƒ¹æ ¼
def merge_with_date_condition(orders, prices):
    """æ ¹æ“šè¨‚å–®æ—¥æœŸåœ¨åƒ¹æ ¼å€é–“å…§é€²è¡Œæ¢ä»¶å¼åˆä½µ"""
    merged_data = []
    
    for _, order in orders.iterrows():
        order_date = order['order_date']
        product_id = order['product_id']
        
        # æ‰¾å‡ºé©ç”¨çš„åƒ¹æ ¼è¨˜éŒ„
        applicable_price = prices[
            (prices['product_id'] == product_id) & 
            (prices['start_date'] <= order_date) & 
            (prices['end_date'] >= order_date)
        ]
        
        if not applicable_price.empty:
            # å°‡è¨‚å–®èˆ‡åƒ¹æ ¼è³‡è¨Šåˆä½µ
            order_dict = order.to_dict()
            order_dict.update({
                'unit_price': applicable_price.iloc[0]['unit_price'],
                'price_start_date': applicable_price.iloc[0]['start_date'],
                'price_end_date': applicable_price.iloc[0]['end_date']
            })
            merged_data.append(order_dict)
        else:
            # æ²’æœ‰æ‰¾åˆ°é©ç”¨çš„åƒ¹æ ¼
            order_dict = order.to_dict()
            order_dict.update({
                'unit_price': None,
                'price_start_date': None,
                'price_end_date': None
            })
            merged_data.append(order_dict)
    
    return pd.DataFrame(merged_data)

# åŸ·è¡Œæ¢ä»¶å¼åˆä½µ
@timing_decorator
def perform_conditional_merge():
    return merge_with_date_condition(sample_orders, price_history)

conditional_merged = perform_conditional_merge()
print("\næ¢ä»¶å¼åˆä½µçµæœ:")
print(conditional_merged[['order_id', 'product_id', 'order_date', 'quantity', 'unit_price']])
conditional_merged['total_amount'] = conditional_merged['quantity'] * conditional_merged['unit_price']
print("\nè¨ˆç®—ç¸½é‡‘é¡å¾Œ:")
print(conditional_merged[['order_id', 'product_id', 'quantity', 'unit_price', 'total_amount']])

# %% [markdown]
# ### 6.2 ä½¿ç”¨ pd.merge_asof é€²è¡Œè¿‘ä¼¼åˆä½µ

# %%
# ä½¿ç”¨ merge_asof é€²è¡Œè¿‘ä¼¼åˆä½µ (é¡ä¼¼è³‡æ–™åº«ä¸­çš„è¿‘ä¼¼é€£æ¥)
print("è¿‘ä¼¼åˆä½µ (Merge Asof)")

# å‰µå»ºè‚¡ç¥¨åƒ¹æ ¼æ™‚é–“åºåˆ—
np.random.seed(42)
stock_prices = pd.DataFrame({
    'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
    'stock_price': np.random.randn(10).cumsum() + 100  # æ¨¡æ“¬è‚¡ç¥¨åƒ¹æ ¼
})

# å‰µå»ºè¨‚å–®æ™‚é–“åºåˆ— (æ™‚é–“èˆ‡è‚¡ç¥¨åƒ¹æ ¼ä¸å®Œå…¨åŒ¹é…)
trade_orders = pd.DataFrame({
    'timestamp': pd.to_datetime([
        '2023-01-01 12:30:00', '2023-01-02 09:15:00', 
        '2023-01-03 16:45:00', '2023-01-05 10:30:00',
        '2023-01-07 14:20:00', '2023-01-08 11:05:00'
    ]),
    'order_id': ['A001', 'A002', 'A003', 'A004', 'A005', 'A006'],
    'quantity': [100, 150, 200, 120, 180, 250]
})

print("è‚¡ç¥¨åƒ¹æ ¼æ™‚é–“åºåˆ—:")
print(stock_prices)
print("\nè¨‚å–®æ™‚é–“åºåˆ—:")
print(trade_orders)

# ç¢ºä¿æ•¸æ“šå·²æ’åº (merge_asof è¦æ±‚æŒ‰åˆä½µéµæ’åº)
stock_prices = stock_prices.sort_values('timestamp')
trade_orders = trade_orders.sort_values('timestamp')

# ä½¿ç”¨ merge_asof é€²è¡Œè¿‘ä¼¼åˆä½µ
# é€™æœƒæ‰¾åˆ°æ¯å€‹è¨‚å–®æ™‚é–“ä¹‹å‰æœ€è¿‘çš„è‚¡ç¥¨åƒ¹æ ¼
asof_merged = pd.merge_asof(
    trade_orders,
    stock_prices,
    on='timestamp',
    direction='backward'  # ä½¿ç”¨ä¹‹å‰æœ€è¿‘çš„åƒ¹æ ¼ (å¯æ”¹ç‚º 'forward' æˆ– 'nearest')
)

print("\nmerge_asof è¿‘ä¼¼åˆä½µçµæœ:")
print(asof_merged)

# è¨ˆç®—è¨‚å–®å€¼
asof_merged['order_value'] = asof_merged['quantity'] * asof_merged['stock_price']
print("\nè¨ˆç®—è¨‚å–®åƒ¹å€¼:")
print(asof_merged[['order_id', 'timestamp', 'quantity', 'stock_price', 'order_value']])

# %% [markdown]
# ### 6.3 åˆ†å±¤æ•¸æ“šçš„åˆä½µ (éšå±¤å¼æ•¸æ“š)

# %%
# æ¼”ç¤ºè™•ç†åˆ†å±¤æ•¸æ“šçš„åˆä½µ
print("éšå±¤å¼æ•¸æ“šåˆä½µ")

# å‰µå»ºçµ„ç¹”çµæ§‹æ•¸æ“š
org_hierarchy = pd.DataFrame({
    'employee_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
    'name': ['John', 'Emma', 'Michael', 'Sophia', 'William', 'Olivia', 'James', 'Ava', 'Benjamin', 'Mia'],
    'department': ['Sales', 'IT', 'Marketing', 'Sales', 'Finance', 'IT', 'Marketing', 'Finance', 'Sales', 'IT'],
    'manager_id': [None, 101, 101, 101, 102, 102, 103, 105, 104, 106]  # None è¡¨ç¤º CEO
})

# å‰µå»ºæ¥­ç¸¾æ•¸æ“š
performance = pd.DataFrame({
    'employee_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
    'sales': [150000, 0, 120000, 180000, 0, 0, 90000, 0, 200000, 0],
    'rating': [4.5, 4.8, 4.0, 4.7, 4.2, 4.9, 3.8, 4.1, 4.6, 4.3]
})

print("çµ„ç¹”çµæ§‹:")
print(org_hierarchy)
print("\næ¥­ç¸¾æ•¸æ“š:")
print(performance)

# åŸºæœ¬åˆä½µï¼šå“¡å·¥èˆ‡æ¥­ç¸¾
employee_performance = pd.merge(
    org_hierarchy,
    performance,
    on='employee_id'
)

print("\nå“¡å·¥æ¥­ç¸¾åŸºæœ¬åˆä½µ:")
print(employee_performance)

# éšå±¤åˆä½µï¼šæŸ¥æ‰¾æ¯å€‹å“¡å·¥çš„ç¶“ç†åŠå…¶æ¥­ç¸¾
# é¦–å…ˆï¼Œå‰µå»ºç¶“ç† DataFrame
managers = org_hierarchy.rename(columns={
    'employee_id': 'manager_id',
    'name': 'manager_name',
    'department': 'manager_department'
})

# ç§»é™¤ä¸å¿…è¦çš„åˆ—
managers = managers.drop('manager_id', axis=1)

# åˆä½µå“¡å·¥èˆ‡å…¶ç¶“ç†è³‡è¨Š
employees_with_managers = pd.merge(
    employee_performance,
    managers,
    on='manager_id',
    how='left'  # ä¿ç•™æ‰€æœ‰å“¡å·¥ï¼ŒåŒ…æ‹¬æ²’æœ‰ç¶“ç†çš„äºº
)

print("\nå“¡å·¥èˆ‡ç¶“ç†éšå±¤åˆä½µ:")
print(employees_with_managers[['employee_id', 'name', 'manager_id', 'manager_name', 'sales', 'rating']])

# è¨ˆç®—æ¯å€‹ç¶“ç†çš„åœ˜éšŠç¸½æ¥­ç¸¾
team_performance = employees_with_managers.groupby('manager_id').agg({
    'sales': 'sum',
    'rating': 'mean'
}).reset_index().rename(columns={'sales': 'team_sales', 'rating': 'team_avg_rating'})

# åˆä½µç¶“ç†èˆ‡åœ˜éšŠæ¥­ç¸¾
manager_performance = pd.merge(
    org_hierarchy,
    team_performance,
    left_on='employee_id',
    right_on='manager_id',
    how='left'
)

print("\nç¶“ç†çš„åœ˜éšŠæ¥­ç¸¾:")
print(manager_performance[['employee_id', 'name', 'team_sales', 'team_avg_rating']])

# %% [markdown]
# ## ğŸ“Š 7. æ¥­å‹™æ™ºèƒ½å¯¦éš›æ¡ˆä¾‹

# %% [markdown]
# ### 7.1 éŠ·å”®æ•¸æ“šå¤šç¶­åˆ†æ

# %%
# ä½¿ç”¨ä¹‹å‰å‰µå»ºçš„å®Œæ•´è¨‚å–®è¦–åœ–é€²è¡Œå¤šç¶­éŠ·å”®åˆ†æ
print("éŠ·å”®æ•¸æ“šå¤šç¶­åˆ†æ")

# ç¢ºä¿æˆ‘å€‘æœ‰å®Œæ•´è¨‚å–®è¦–åœ–æ•¸æ“š
if 'full_orders' in locals():
    # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„éŠ·å”®ç¸½è¨ˆ
    category_sales = full_orders.groupby('category_name').agg({
        'item_price': 'sum',
        'order_id': 'nunique',
        'customer_id': 'nunique'
    }).reset_index()
    
    category_sales.columns = ['é¡åˆ¥', 'éŠ·å”®ç¸½é¡', 'è¨‚å–®æ•¸', 'å®¢æˆ¶æ•¸']
    category_sales['å¹³å‡è¨‚å–®é‡‘é¡'] = category_sales['éŠ·å”®ç¸½é¡'] / category_sales['è¨‚å–®æ•¸']
    category_sales['å®¢å–®åƒ¹'] = category_sales['éŠ·å”®ç¸½é¡'] / category_sales['å®¢æˆ¶æ•¸']
    
    print("é¡åˆ¥éŠ·å”®åˆ†æ:")
    print(category_sales)
    
    # æŒ‰éŠ·å”®ä»£è¡¨å’Œåœ°å€çš„éŠ·å”®åˆ†æ
    region_sales_rep = full_orders.groupby(['region', 'sales_rep_name']).agg({
        'item_price': 'sum',
        'order_id': 'nunique',
        'customer_id': 'nunique'
    }).reset_index()
    
    region_sales_rep.columns = ['åœ°å€', 'éŠ·å”®ä»£è¡¨', 'éŠ·å”®ç¸½é¡', 'è¨‚å–®æ•¸', 'å®¢æˆ¶æ•¸']
    region_sales_rep['å¹³å‡è¨‚å–®é‡‘é¡'] = region_sales_rep['éŠ·å”®ç¸½é¡'] / region_sales_rep['è¨‚å–®æ•¸']
    
    print("\nåœ°å€èˆ‡éŠ·å”®ä»£è¡¨ç¸¾æ•ˆ:")
    print(region_sales_rep.sort_values('éŠ·å”®ç¸½é¡', ascending=False).head(10))
    
    # æŒ‰é¡åˆ¥å’Œå®¢æˆ¶é¡å‹çš„äº¤å‰åˆ†æ
    category_customer_type = full_orders.groupby(['category_name', 'customer_type']).agg({
        'item_price': 'sum'
    }).reset_index()
    
    # æ•¸æ“šé€è¦–è¡¨è½‰æ›
    category_customer_pivot = category_customer_type.pivot(
        index='category_name',
        columns='customer_type',
        values='item_price'
    ).fillna(0)
    
    # æ·»åŠ ç¸½è¨ˆåˆ—
    category_customer_pivot['ç¸½è¨ˆ'] = category_customer_pivot.sum(axis=1)
    
    # è¨ˆç®—ç™¾åˆ†æ¯”
    for col in category_customer_pivot.columns:
        if col != 'ç¸½è¨ˆ':
            category_customer_pivot[f'{col} %'] = category_customer_pivot[col] / category_customer_pivot['ç¸½è¨ˆ'] * 100
    
    print("\né¡åˆ¥èˆ‡å®¢æˆ¶é¡å‹äº¤å‰åˆ†æ:")
    print(category_customer_pivot)
    
    # å¯è¦–åŒ–é¡åˆ¥éŠ·å”®
    plt.figure(figsize=(12, 6))
    plt.bar(category_sales['é¡åˆ¥'], category_sales['éŠ·å”®ç¸½é¡'])
    plt.title('å„é¡åˆ¥éŠ·å”®ç¸½é¡')
    plt.xlabel('ç”¢å“é¡åˆ¥')
    plt.ylabel('éŠ·å”®ç¸½é¡')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()
else:
    print("å°šæœªå‰µå»ºå®Œæ•´è¨‚å–®è¦–åœ–ï¼Œè«‹ç¢ºä¿åŸ·è¡Œå‰é¢çš„ä»£ç¢¼")

# %% [markdown]
# ### 7.2 ä¾›æ‡‰éˆåˆ†æ

# %%
# ä¾›æ‡‰éˆåˆ†æ - ç”¢å“ä¾›æ‡‰å•†ç¸¾æ•ˆè©•ä¼°
print("ä¾›æ‡‰éˆåˆ†æ")

# å‰µå»ºç”¢å“ä¾›æ‡‰æ™‚é–“æ•¸æ“š
np.random.seed(42)
order_dates = pd.date_range('2023-01-01', periods=300, freq='D')
product_ids = np.random.choice(products['product_id'].values, 300)
supplier_ids = [products[products['product_id'] == pid]['supplier_id'].values[0] for pid in product_ids]

supply_chain_data = pd.DataFrame({
    'order_date': order_dates,
    'product_id': product_ids,
    'supplier_id': supplier_ids,
    'order_quantity': np.random.randint(10, 100, 300),
    'lead_time_days': np.random.randint(1, 30, 300),  # ä¾›æ‡‰å•†ä¾›è²¨å¤©æ•¸
    'defect_rate': np.random.uniform(0, 0.05, 300)   # ç”¢å“ç¼ºé™·ç‡
})

# åˆä½µä¾›æ‡‰å•†è³‡è¨Š
supply_chain_with_supplier = pd.merge(
    supply_chain_data,
    suppliers,
    on='supplier_id'
)

# åˆä½µç”¢å“è³‡è¨Š
supply_chain_full = pd.merge(
    supply_chain_with_supplier,
    products[['product_id', 'product_name', 'category_id']],
    on='product_id'
)

# åˆä½µé¡åˆ¥è³‡è¨Š
supply_chain_full = pd.merge(
    supply_chain_full,
    categories,
    on='category_id'
)

print("ä¾›æ‡‰éˆæ•¸æ“š:")
print(supply_chain_full.head())

# ä¾›æ‡‰å•†ç¸¾æ•ˆè©•ä¼°
supplier_performance = supply_chain_full.groupby('supplier_name').agg({
    'lead_time_days': ['mean', 'min', 'max', 'std'],
    'defect_rate': ['mean', 'max'],
    'order_quantity': 'sum',
    'product_id': 'nunique'
})

# é™ç´šå¤šç´šåˆ—ç´¢å¼•
supplier_performance.columns = ['å¹³å‡ä¾›è²¨å¤©æ•¸', 'æœ€çŸ­ä¾›è²¨å¤©æ•¸', 'æœ€é•·ä¾›è²¨å¤©æ•¸', 'ä¾›è²¨å¤©æ•¸æ¨™æº–å·®', 
                              'å¹³å‡ç¼ºé™·ç‡', 'æœ€é«˜ç¼ºé™·ç‡', 'è¨‚è³¼ç¸½é‡', 'ç”¢å“ç¨®é¡æ•¸']

# å‰µå»ºç¶œåˆè©•åˆ†
supplier_performance['è©•åˆ†'] = (
    (1 / supplier_performance['å¹³å‡ä¾›è²¨å¤©æ•¸']) * 30 +  # ä¾›è²¨å¤©æ•¸è¶ŠçŸ­è¶Šå¥½
    (1 - supplier_performance['å¹³å‡ç¼ºé™·ç‡']) * 50 +    # ç¼ºé™·ç‡è¶Šä½è¶Šå¥½
    np.log10(supplier_performance['è¨‚è³¼ç¸½é‡']) * 10 +  # è¨‚è³¼é‡å¤§ä½†å½±éŸ¿è¼ƒå°
    np.log10(supplier_performance['ç”¢å“ç¨®é¡æ•¸']) * 10  # ç”¢å“å¤šæ¨£æ€§ä¹Ÿè€ƒæ…®
)

# æ’åº
supplier_performance = supplier_performance.sort_values('è©•åˆ†', ascending=False)

print("\nä¾›æ‡‰å•†ç¸¾æ•ˆè©•ä¼°:")
print(supplier_performance)

# å¯è¦–åŒ–ä¾›æ‡‰å•†è©•æ¯”
plt.figure(figsize=(12, 6))
supplier_performance['è©•åˆ†'].plot(kind='bar')
plt.title('ä¾›æ‡‰å•†ç¸¾æ•ˆè©•åˆ†')
plt.xlabel('ä¾›æ‡‰å•†')
plt.ylabel('è©•åˆ†')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 7.3 å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼åˆ†æ

# %%
# å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼åˆ†æ (CLV)
print("å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼åˆ†æ")

# å‡è¨­æˆ‘å€‘å·²æœ‰å®Œæ•´çš„è¨‚å–®è³‡æ–™
if 'full_orders' in locals():
    # æ·»åŠ è¨‚å–®å¹´ä»½åˆ—
    full_orders['order_year'] = full_orders['order_date'].dt.year
    
    # è¨ˆç®—æ¯å€‹å®¢æˆ¶æ¯å¹´çš„æ¶ˆè²»
    customer_yearly_spending = full_orders.groupby(['customer_id', 'customer_name', 'order_year']).agg({
        'item_price': 'sum',
        'order_id': 'nunique'
    }).reset_index()
    
    customer_yearly_spending.columns = ['å®¢æˆ¶ID', 'å®¢æˆ¶åç¨±', 'å¹´ä»½', 'å¹´åº¦æ¶ˆè²»', 'è¨‚å–®æ•¸é‡']
    
    # è¨ˆç®—å¹³å‡è¨‚å–®é‡‘é¡
    customer_yearly_spending['å¹³å‡è¨‚å–®é‡‘é¡'] = customer_yearly_spending['å¹´åº¦æ¶ˆè²»'] / customer_yearly_spending['è¨‚å–®æ•¸é‡']
    
    # è¨ˆç®—å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼ (ç°¡åŒ–ç‰ˆ - å¹³å‡å¹´åº¦æ¶ˆè²» * é æœŸå®¢æˆ¶å¹´é™)
    customer_avg_yearly = customer_yearly_spending.groupby(['å®¢æˆ¶ID', 'å®¢æˆ¶åç¨±']).agg({
        'å¹´åº¦æ¶ˆè²»': 'mean',
        'è¨‚å–®æ•¸é‡': 'mean',
        'å¹³å‡è¨‚å–®é‡‘é¡': 'mean'
    }).reset_index()
    
    # å‡è¨­å¹³å‡å®¢æˆ¶é—œä¿‚ç¶­æŒ5å¹´ (å¯¦éš›æ‡‰ä½¿ç”¨æ›´è¤‡é›œçš„å­˜æ´»åˆ†æ)
    expected_years = 5
    customer_avg_yearly['é ä¼°ç”Ÿå‘½é€±æœŸåƒ¹å€¼'] = customer_avg_yearly['å¹´åº¦æ¶ˆè²»'] * expected_years
    
    # å°å®¢æˆ¶é€²è¡Œç´°åˆ† (åŸºæ–¼CLV)
    def clv_segment(clv):
        if clv >= 50000:
            return 'A - é«˜åƒ¹å€¼'
        elif clv >= 20000:
            return 'B - ä¸­é«˜åƒ¹å€¼'
        elif clv >= 5000:
            return 'C - ä¸­åƒ¹å€¼'
        else:
            return 'D - ä½åƒ¹å€¼'
    
    customer_avg_yearly['å®¢æˆ¶ç´°åˆ†'] = customer_avg_yearly['é ä¼°ç”Ÿå‘½é€±æœŸåƒ¹å€¼'].apply(clv_segment)
    
    # æŒ‰ç…§CLVæ’åº
    customer_avg_yearly = customer_avg_yearly.sort_values('é ä¼°ç”Ÿå‘½é€±æœŸåƒ¹å€¼', ascending=False)
    
    print("å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼ (CLV) åˆ†æ:")
    print(customer_avg_yearly.head(10))
    
    # è¨ˆç®—æ¯å€‹ç´°åˆ†çš„å®¢æˆ¶æ•¸é‡å’Œç¸½CLV
    segment_analysis = customer_avg_yearly.groupby('å®¢æˆ¶ç´°åˆ†').agg({
        'å®¢æˆ¶ID': 'count',
        'é ä¼°ç”Ÿå‘½é€±æœŸåƒ¹å€¼': 'sum'
    }).reset_index()
    
    segment_analysis.columns = ['å®¢æˆ¶ç´°åˆ†', 'å®¢æˆ¶æ•¸é‡', 'ç¸½CLV']
    segment_analysis['å¹³å‡CLV'] = segment_analysis['ç¸½CLV'] / segment_analysis['å®¢æˆ¶æ•¸é‡']
    segment_analysis['å®¢æˆ¶ä½”æ¯”'] = segment_analysis['å®¢æˆ¶æ•¸é‡'] / segment_analysis['å®¢æˆ¶æ•¸é‡'].sum() * 100
    segment_analysis['æ”¶å…¥ä½”æ¯”'] = segment_analysis['ç¸½CLV'] / segment_analysis['ç¸½CLV'].sum() * 100
    
    print("\nå®¢æˆ¶ç´°åˆ†åˆ†æ:")
    print(segment_analysis)
    
    # ç¹ªè£½å®¢æˆ¶ç´°åˆ†ä½”æ¯”åœ–
    plt.figure(figsize=(10, 6))
    plt.pie(segment_analysis['å®¢æˆ¶æ•¸é‡'], labels=segment_analysis['å®¢æˆ¶ç´°åˆ†'], 
            autopct='%1.1f%%', startangle=90)
    plt.title('å®¢æˆ¶åˆ†ç¾¤ä½”æ¯”')
    plt.axis('equal')
    plt.show()
    
    # ç¹ªè£½å®¢æˆ¶æ”¶å…¥ä½”æ¯”åœ–
    plt.figure(figsize=(10, 6))
    plt.pie(segment_analysis['ç¸½CLV'], labels=segment_analysis['å®¢æˆ¶ç´°åˆ†'], 
            autopct='%1.1f%%', startangle=90)
    plt.title('å®¢æˆ¶æ”¶å…¥ä½”æ¯”')
    plt.axis('equal')
    plt.show()
else:
    print("å°šæœªå‰µå»ºå®Œæ•´è¨‚å–®è¦–åœ–ï¼Œè«‹ç¢ºä¿åŸ·è¡Œå‰é¢çš„ä»£ç¢¼")

# %% [markdown]
# ## ğŸ“Š 8. åˆä½µçš„æœ€ä½³å¯¦è¸èˆ‡ç¸½çµ

# %%
# æ•´ç†åˆä½µæ“ä½œçš„æœ€ä½³å¯¦è¸
merge_best_practices = pd.DataFrame({
    'é ˜åŸŸ': [
        'æ•¸æ“šæº–å‚™', 'æ•¸æ“šæº–å‚™', 'æ€§èƒ½å„ªåŒ–', 'æ€§èƒ½å„ªåŒ–', 'æ€§èƒ½å„ªåŒ–',
        'æ•¸æ“šé©—è­‰', 'æ•¸æ“šé©—è­‰', 'é«˜éšæŠ€å·§', 'é«˜éšæŠ€å·§', 'é«˜éšæŠ€å·§'
    ],
    'æœ€ä½³å¯¦è¸': [
        'åˆä½µå‰æª¢æŸ¥éµçš„åˆ†å¸ƒ', 'ç¢ºä¿åˆä½µéµæ²’æœ‰æ„å¤–çš„ç¼ºå¤±å€¼', 'å°å¤§å‹æ•¸æ“šé›†å…ˆæ’åºå†åˆä½µ', 
        'ä½¿ç”¨åˆ†å¡Šç­–ç•¥è™•ç†è¶…å¤§æ•¸æ“šé›†', 'è€ƒæ…®å°‡å­—ç¬¦ä¸²é¡å‹çš„éµè½‰æ›ç‚ºé¡åˆ¥(category)å‹åˆ¥',
        'é©—è­‰åˆä½µå¾Œçš„è¡Œæ•¸æ˜¯å¦ç¬¦åˆé æœŸ', 'æª¢æŸ¥åˆä½µå¾ŒèšåˆæŒ‡æ¨™æ˜¯å¦ä¿æŒä¸€è‡´',
        'å¤šè¡¨åˆä½µæ™‚ä½¿ç”¨é€æ­¥åˆä½µï¼Œæª¢æŸ¥æ¯æ­¥çµæœ', 'ä½¿ç”¨ merge_asof è™•ç†æ™‚é–“ç›¸è¿‘ä½†ä¸å®Œå…¨åŒ¹é…çš„æƒ…æ³',
        'åˆ©ç”¨ SQL è™•ç†éå¸¸è¤‡é›œçš„åˆä½µé‚è¼¯'
    ],
    'èªªæ˜': [
        'åˆ†ææ¯å€‹åˆä½µéµçš„å”¯ä¸€å€¼æ•¸é‡å’Œé‡è¤‡æƒ…æ³ï¼Œé¿å…æ„å¤–çš„ç¬›å¡çˆ¾ç©',
        'ç¼ºå¤±å€¼åœ¨åˆä½µæ™‚çš„è¡Œç‚ºå¯èƒ½ä¸å¦‚é æœŸï¼Œç‰¹åˆ¥æ˜¯åœ¨å¤–éƒ¨åˆä½µæ™‚',
        'å°æ’åºå¥½çš„æ•¸æ“šé€²è¡Œåˆä½µå¯ä»¥é¡¯è‘—æé«˜å¤§å‹æ•¸æ“šé›†çš„åˆä½µæ€§èƒ½',
        'å°‡å¤§å‹æ•¸æ“šé›†åˆ†å‰²æˆå°å¡Šé€å€‹åˆä½µï¼Œå¯ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨',
        'å°æ–¼é‡è¤‡å€¼å¤šçš„å­—ä¸²éµï¼Œè½‰æ›ç‚ºcategoryå¯ä»¥ç¯€çœè¨˜æ†¶é«”ä¸¦åŠ é€Ÿåˆä½µ',
        'æª¢æŸ¥åˆä½µå¾Œçš„æ•¸æ“šé‡æ˜¯å¦ç¬¦åˆé æœŸï¼Œç‰¹åˆ¥æ˜¯æœ‰é‡è¤‡éµçš„æƒ…æ³',
        'åˆä½µå¾Œï¼Œç¢ºä¿é—œéµçš„ç¸½è¨ˆã€è¨ˆæ•¸ç­‰èšåˆæŒ‡æ¨™ä¿æŒæ­£ç¢º',
        'è¤‡é›œçš„å¤šè¡¨åˆä½µæ‡‰åˆ†æ­¥é©ŸåŸ·è¡Œï¼Œæ¯æ­¥éƒ½æª¢æŸ¥çµæœçš„æ­£ç¢ºæ€§',
        'é‡‘èã€ç‰©è¯ç¶²ç­‰é ˜åŸŸå¸¸éœ€è¦å°‡äº‹ä»¶èˆ‡æœ€è¿‘çš„åƒè€ƒæ•¸æ“šåŒ¹é…',
        'å°æ–¼ç‰¹åˆ¥è¤‡é›œçš„åˆä½µé‚è¼¯ï¼Œä½¿ç”¨SQLå¯èƒ½æ›´ç‚ºç›´è§€ä¸”é«˜æ•ˆ'
    ]
})

print("åˆä½µæ“ä½œæœ€ä½³å¯¦è¸:")
print(merge_best_practices)

# %% [markdown]
# **èª²ç¨‹ç¸½çµ**ï¼š
# 
# æœ¬èª²ç¨‹æ·±å…¥æ¢è¨äº† Pandas ä¸­çš„é€²éšæ•¸æ“šåˆä½µèˆ‡é€£æ¥æŠ€è¡“ï¼Œæ¶µè“‹çš„é—œéµå…§å®¹åŒ…æ‹¬ï¼š
# 
# 1. **è¤‡é›œæ•¸æ“šé—œä¿‚è™•ç†**
#    - å¤šè¡¨é—œè¯åˆ†æå’Œæ•¸æ“šé‡è®ŠåŒ–çš„ç†è§£
#    - é¿å…ç¬›å¡çˆ¾ç©å•é¡Œçš„ç­–ç•¥
# 
# 2. **åˆä½µé©—è­‰èˆ‡è¨ºæ–·æŠ€è¡“**
#    - é©—è­‰åˆä½µæ“ä½œçš„å®Œæ•´æ€§
#    - åˆä½µå¾Œæ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
# 
# 3. **é«˜æ€§èƒ½åˆä½µæŠ€è¡“**
#    - å¤§æ•¸æ“šé›†åˆä½µå„ªåŒ–æ–¹æ³•
#    - ä½¿ç”¨ SQL è™•ç†è¶…å¤§æ•¸æ“šé›†
# 
# 4. **é€²éšç‰¹æ®Šåˆä½µæŠ€è¡“**
#    - æ¢ä»¶å¼åˆä½µ
#    - ä½¿ç”¨ pd.merge_asof é€²è¡Œè¿‘ä¼¼åˆä½µ
#    - åˆ†å±¤æ•¸æ“šçš„åˆä½µ
# 
# 5. **æ¥­å‹™æ™ºèƒ½å¯¦éš›æ¡ˆä¾‹**
#    - éŠ·å”®æ•¸æ“šå¤šç¶­åˆ†æ
#    - ä¾›æ‡‰éˆåˆ†æ
#    - å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼åˆ†æ
# 
# æŒæ¡é€™äº›é€²éšæŠ€è¡“ï¼Œèƒ½å¤ å¹«åŠ©æ•¸æ“šåˆ†æå¸«å’Œæ•¸æ“šå·¥ç¨‹å¸«è™•ç†è¤‡é›œçš„æ•¸æ“šæ•´åˆä»»å‹™ï¼Œæé«˜æ•¸æ“šè™•ç†çš„æ•ˆç‡å’Œæº–ç¢ºæ€§ï¼Œå¾è€Œç‚ºæ¥­å‹™æ±ºç­–æä¾›æ›´å…¨é¢ã€æ›´æ·±å…¥çš„æ•¸æ“šæ”¯æŒã€‚ 