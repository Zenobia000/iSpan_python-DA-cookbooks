# %% [markdown]
# # ğŸ“˜ M3.5 Pandas æ™‚é–“åºåˆ—èˆ‡è²¡å‹™åˆ†æé€²éšæ‡‰ç”¨
# 
# æœ¬æ•™å­¸å°‡æ·±å…¥æ¢è¨ Pandas åœ¨æ™‚é–“åºåˆ—æ•¸æ“šè™•ç†èˆ‡åˆ†æä¸­çš„é€²éšæ‡‰ç”¨ã€‚æ™‚é–“åºåˆ—æ•¸æ“šåœ¨é‡‘èã€ç¶“æ¿Ÿã€éŠ·å”®é æ¸¬ã€ç¶²ç«™æµé‡åˆ†æç­‰é ˜åŸŸæœ‰è‘—å»£æ³›çš„æ‡‰ç”¨ï¼ŒæŒæ¡é«˜æ•ˆçš„æ™‚é–“åºåˆ—è™•ç†æŠ€è¡“å°æ–¼æ•¸æ“šç§‘å­¸å®¶å’Œåˆ†æå¸«è‡³é—œé‡è¦ã€‚

# %% [markdown]
# ## ğŸ¯ æ•™å­¸ç›®æ¨™
# 
# - ğŸ” æ·±å…¥æŒæ¡æ™‚é–“åºåˆ—æ•¸æ“šçš„é«˜ç´šç‰¹æ€§èˆ‡æ“ä½œæ–¹æ³•
# - ğŸ“ˆ å­¸ç¿’æ™‚é–“åºåˆ—çš„å­£ç¯€æ€§åˆ†è§£ã€è¶¨å‹¢è­˜åˆ¥èˆ‡ç•°å¸¸æª¢æ¸¬
# - ğŸ§® æ¢ç´¢ç§»å‹•çª—å£å‡½æ•¸èˆ‡è‡ªå®šç¾©çª—å£è¨ˆç®—æŠ€è¡“
# - ğŸ“Š æŒæ¡è²¡å‹™æ™‚é–“åºåˆ—åˆ†æä¸­çš„é—œéµæŒ‡æ¨™è¨ˆç®—
# - ğŸ“‰ é‹ç”¨æ™‚é–“åºåˆ—åˆ†æè§£æ±ºå¯¦éš›æ¥­å‹™å•é¡Œ
# - ğŸ”® å¯¦ç¾åŸºæ–¼æ­·å²æ•¸æ“šçš„æ™‚é–“åºåˆ—é æ¸¬æ¨¡å‹

# %% [markdown]
# ## ğŸ§° 1. ç’°å¢ƒè¨­ç½®èˆ‡æ•¸æ“šæº–å‚™

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller, acf, pacf
from scipy import stats
import warnings
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error
import statsmodels.api as sm

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)

# è¨­ç½®å¯è¦–åŒ–é¢¨æ ¼
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 12

# è¨­ç½®é¡¯ç¤ºé¸é …
pd.set_option('display.max_rows', 15)
pd.set_option('display.max_columns', 12)
pd.set_option('display.width', 100)
pd.set_option('display.precision', 2)
pd.set_option('display.float_format', '{:.2f}'.format)

# è¨­ç½®éš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯é‡ç¾
np.random.seed(42)

# å‰µå»ºæ¨¡æ“¬æ™‚é–“åºåˆ—æ•¸æ“šç”Ÿæˆå‡½æ•¸
def generate_time_series(start_date, periods, freq='D', trend=0.2, seasonality=True, 
                         cycle_period=12, noise_level=0.05, missing_pct=0):
    """
    ç”Ÿæˆç”¨æ–¼æ•™å­¸çš„æ¨¡æ“¬æ™‚é–“åºåˆ—æ•¸æ“š
    
    åƒæ•¸:
    - start_date: é–‹å§‹æ—¥æœŸ
    - periods: æ•¸æ“šé»æ•¸é‡
    - freq: æ™‚é–“é »ç‡ (D-æ—¥, B-å·¥ä½œæ—¥, W-é€±, M-æœˆ, Q-å­£, Y-å¹´)
    - trend: ç·šæ€§è¶¨å‹¢å¼·åº¦
    - seasonality: æ˜¯å¦åŒ…å«å­£ç¯€æ€§çµ„ä»¶
    - cycle_period: å­£ç¯€æ€§é€±æœŸ
    - noise_level: å™ªè²å¼·åº¦
    - missing_pct: ç¼ºå¤±å€¼ç™¾åˆ†æ¯”
    
    è¿”å›:
    - åŒ…å«æ¨¡æ“¬æ•¸æ“šçš„ Pandas Series
    """
    dates = pd.date_range(start=start_date, periods=periods, freq=freq)
    
    # å‰µå»ºè¶¨å‹¢çµ„ä»¶
    trend_component = np.arange(periods) * trend
    
    # å‰µå»ºå­£ç¯€æ€§çµ„ä»¶
    if seasonality:
        seasonal_component = np.sin(np.arange(periods) * (2 * np.pi / cycle_period)) * 10
    else:
        seasonal_component = np.zeros(periods)
    
    # å‰µå»ºå™ªè²çµ„ä»¶
    noise_component = np.random.normal(0, noise_level * 10, periods)
    
    # åˆä½µçµ„ä»¶
    data = 100 + trend_component + seasonal_component + noise_component
    
    # æ·»åŠ ç¼ºå¤±å€¼
    if missing_pct > 0:
        n_missing = int(periods * missing_pct / 100)
        missing_idx = np.random.choice(periods, n_missing, replace=False)
        data[missing_idx] = np.nan
    
    # å‰µå»ºæ™‚é–“åºåˆ—
    ts = pd.Series(data, index=dates, name='value')
    return ts

# %% [markdown]
# ## ğŸ“Š 2. æ™‚é–“åºåˆ—æ•¸æ“šçš„é€²éšæ“ä½œ

# %%
# ç”Ÿæˆä¸‰ç¨®ä¸åŒç‰¹æ€§çš„æ™‚é–“åºåˆ—æ•¸æ“š
daily_sales = generate_time_series('2022-01-01', 365, freq='D', trend=0.1, 
                                  seasonality=True, cycle_period=30)
monthly_revenue = generate_time_series('2015-01-01', 96, freq='M', trend=0.5, 
                                      seasonality=True, cycle_period=12)
hourly_web_traffic = generate_time_series('2023-01-01', 168, freq='H', trend=0.05, 
                                         seasonality=True, cycle_period=24, noise_level=0.2)

# é¡¯ç¤ºæ•¸æ“š
print("æ—¥éŠ·å”®æ•¸æ“š (Daily Sales):")
print(daily_sales.head())
print("\næœˆåº¦æ”¶å…¥æ•¸æ“š (Monthly Revenue):")
print(monthly_revenue.head())
print("\næ¯å°æ™‚ç¶²ç«™æµé‡ (Hourly Web Traffic):")
print(hourly_web_traffic.head())

# å¯è¦–åŒ–ä¸‰å€‹æ™‚é–“åºåˆ—
fig, axes = plt.subplots(3, 1, figsize=(12, 12))

daily_sales.plot(ax=axes[0], title='æ—¥éŠ·å”®æ•¸æ“š (2022å¹´)')
axes[0].set_ylabel('éŠ·å”®é‡')

monthly_revenue.plot(ax=axes[1], title='æœˆåº¦æ”¶å…¥æ•¸æ“š (2015-2023å¹´)')
axes[1].set_ylabel('æ”¶å…¥ (åƒå…ƒ)')

hourly_web_traffic.plot(ax=axes[2], title='æ¯å°æ™‚ç¶²ç«™æµé‡ (2023å¹´1æœˆåˆ)')
axes[2].set_ylabel('è¨ªå•æ¬¡æ•¸')

plt.tight_layout()
plt.show()

# %% [markdown]
# ### 2.1 é€²éšæ™‚é–“åºåˆ—ç´¢å¼•æŠ€è¡“

# %%
# å¾æœˆåº¦æ”¶å…¥æ•¸æ“šä¸­é¸å–ç‰¹å®šæ™‚æœŸçš„æ•¸æ“š
print("2018å¹´æ•´å¹´çš„æœˆåº¦æ”¶å…¥:")
print(monthly_revenue['2018'])

print("\n2019å¹´ç¬¬ä¸€å­£åº¦çš„æœˆåº¦æ”¶å…¥:")
print(monthly_revenue['2019-01':'2019-03'])

# ä½¿ç”¨æ—¥æœŸåç§»é‡é€²è¡Œé¸æ“‡
print("\næœ€è¿‘3å¹´çš„æœˆåº¦æ”¶å…¥:")
current_date = monthly_revenue.index[-1]
three_years_ago = current_date - pd.DateOffset(years=3)
print(monthly_revenue[monthly_revenue.index >= three_years_ago])

# ä½¿ç”¨ç´¢å¼•çš„æ—¥æœŸå±¬æ€§é€²è¡Œç¯©é¸
summer_months = monthly_revenue[monthly_revenue.index.month.isin([6, 7, 8])]
print("\nå¤å­£æœˆä»½çš„å¹³å‡æ”¶å…¥:")
print(summer_months.groupby(summer_months.index.year).mean())

# ä½¿ç”¨æ—¥æœŸç´¢å¼•çš„å¤šç¨®æ–¹æ³•
print("\næ¯å¹´ç¬¬å››å­£åº¦çš„æ”¶å…¥ç¸½å’Œ:")
q4_revenue = monthly_revenue[monthly_revenue.index.quarter == 4]
print(q4_revenue.groupby(q4_revenue.index.year).sum())

# å‰µå»ºåŒ…å«å¤šåˆ—çš„æ™‚é–“åºåˆ—
multi_ts = pd.DataFrame({
    'Sales': daily_sales.values,
    'Website_Visits': daily_sales.values * 5 + np.random.normal(0, 10, len(daily_sales)),
    'Marketing_Spend': daily_sales.values * 0.5 + np.random.normal(0, 5, len(daily_sales))
}, index=daily_sales.index)

print("\nå¤šè®Šé‡æ™‚é–“åºåˆ—æ•¸æ“š:")
print(multi_ts.head())

# è¨ˆç®—æ—¥æœŸä¹‹é–“çš„é–“éš”
latest_date = monthly_revenue.index[-1]
earliest_date = monthly_revenue.index[0]
date_range = latest_date - earliest_date
print(f"\næ™‚é–“åºåˆ—è·¨åº¦: {date_range.days} å¤© (ç´„ {date_range.days/365.25:.1f} å¹´)")

# %% [markdown]
# ### 2.2 è™•ç†æ™‚é–“æˆ³èˆ‡æ™‚å€

# %%
# å‰µå»ºåŒ…å«æ™‚å€çš„æ™‚é–“åºåˆ—
timezone_example = pd.DataFrame({
    'UTC': pd.date_range('2023-01-01', periods=24, freq='H'),
    'Value': np.random.normal(100, 10, 24)
})

# è½‰æ›ç‚ºæ™‚å€æ„ŸçŸ¥çš„æ™‚é–“æˆ³
timezone_example['UTC'] = timezone_example['UTC'].dt.tz_localize('UTC')
timezone_example.set_index('UTC', inplace=True)

print("UTCæ™‚é–“çš„æ™‚é–“åºåˆ—:")
print(timezone_example.head())

# è½‰æ›ç‚ºå…¶ä»–æ™‚å€
taipei_time = timezone_example.index.tz_convert('Asia/Taipei')
new_york_time = timezone_example.index.tz_convert('America/New_York')

comparison_df = pd.DataFrame({
    'UTCæ™‚é–“': timezone_example.index,
    'å°åŒ—æ™‚é–“': taipei_time,
    'ç´ç´„æ™‚é–“': new_york_time,
    'æ•¸å€¼': timezone_example['Value'].values
})

print("\nè·¨æ™‚å€æ™‚é–“æ¯”è¼ƒ:")
print(comparison_df.head())

# åœ¨ä¸åŒæ™‚å€ä¹‹é–“è½‰æ›æ•¸æ“š
taipei_series = pd.Series(timezone_example['Value'].values, 
                         index=taipei_time, 
                         name='å°åŒ—æ™‚é–“æ•¸å€¼')

# æŒ‰ç…§ç•¶åœ°æ™‚é–“çš„å°æ™‚åˆ†çµ„
taipei_hourly_avg = taipei_series.groupby(taipei_series.index.hour).mean()
print("\nå°åŒ—æ™‚é–“å„å°æ™‚çš„å¹³å‡å€¼:")
print(taipei_hourly_avg)

# ç¹ªè£½å„æ™‚å€çš„æ•¸æ“š
plt.figure(figsize=(12, 6))
plt.plot(timezone_example.index, timezone_example['Value'], label='UTCæ™‚é–“')
plt.plot(taipei_time, timezone_example['Value'], label='å°åŒ—æ™‚é–“')
plt.legend()
plt.title('ä¸åŒæ™‚å€çš„æ™‚é–“åºåˆ—è¦–è¦ºåŒ–')
plt.xlabel('æ™‚é–“')
plt.ylabel('æ•¸å€¼')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 2.3 æ™‚é–“åºåˆ—çš„ç¼ºå¤±å€¼è™•ç†

# %%
# å‰µå»ºå¸¶æœ‰ç¼ºå¤±å€¼çš„æ™‚é–“åºåˆ—
ts_with_missing = generate_time_series('2022-01-01', 100, freq='D', 
                                      missing_pct=15, noise_level=0.1)

print("å¸¶æœ‰ç¼ºå¤±å€¼çš„æ™‚é–“åºåˆ—:")
print(ts_with_missing.head(10))
print(f"\nç¼ºå¤±å€¼æ•¸é‡: {ts_with_missing.isna().sum()} (ç¸½æ¯”ä¾‹: {ts_with_missing.isna().mean():.1%})")

# å¯è¦–åŒ–ç¼ºå¤±å€¼
plt.figure(figsize=(12, 5))
plt.plot(ts_with_missing.index, ts_with_missing.values)
plt.scatter(ts_with_missing[ts_with_missing.isna()].index, 
           [ts_with_missing.min() - 5] * ts_with_missing.isna().sum(), 
           color='red', label='ç¼ºå¤±å€¼')
plt.legend()
plt.title('æ™‚é–“åºåˆ—æ•¸æ“šä¸­çš„ç¼ºå¤±å€¼')
plt.tight_layout()
plt.show()

# æ¯”è¼ƒå„ç¨®ç¼ºå¤±å€¼è™•ç†æ–¹æ³•
methods = {
    'åŸå§‹æ•¸æ“š (å«ç¼ºå¤±)': ts_with_missing,
    'å‰å‘å¡«å…… (ffill)': ts_with_missing.ffill(),
    'å¾Œå‘å¡«å…… (bfill)': ts_with_missing.bfill(),
    'ç·šæ€§æ’å€¼ (linear)': ts_with_missing.interpolate(method='linear'),
    'å¤šé …å¼æ’å€¼ (polynomial)': ts_with_missing.interpolate(method='polynomial', order=2),
    'æ™‚é–“åŠ æ¬Šæ’å€¼ (time)': ts_with_missing.interpolate(method='time'),
    'æ¨£æ¢æ’å€¼ (spline)': ts_with_missing.interpolate(method='spline', order=3)
}

# å¯è¦–åŒ–æ¯”è¼ƒä¸åŒå¡«å……æ–¹æ³•
plt.figure(figsize=(14, 8))
for i, (name, data) in enumerate(methods.items(), 1):
    plt.subplot(3, 3, i)
    plt.plot(data.index, data.values)
    plt.title(name)
    plt.xticks(rotation=45)
    if i == 1:  # åªåœ¨ç¬¬ä¸€å¼µåœ–ä¸Šæ¨™è¨˜ç¼ºå¤±å€¼
        plt.scatter(ts_with_missing[ts_with_missing.isna()].index, 
                   [ts_with_missing.min() - 5] * ts_with_missing.isna().sum(), 
                   color='red', s=30)
plt.tight_layout()
plt.show()

# è©•ä¼°å„ç¨®æ’å€¼æ–¹æ³•
# é¦–å…ˆå‰µå»ºä¸€å€‹å®Œæ•´çš„åƒè€ƒåºåˆ—
complete_ts = generate_time_series('2022-01-01', 100, freq='D', missing_pct=0)
# ç„¶å¾Œåœ¨ç›¸åŒä½ç½®æ·»åŠ ç¼ºå¤±å€¼
missing_idx = ts_with_missing.isna()
evaluation_ts = complete_ts.copy()
evaluation_ts[missing_idx] = np.nan

# è¨ˆç®—ä¸åŒæ–¹æ³•èˆ‡åŸå§‹å€¼çš„å‡æ–¹èª¤å·® (MSE)
print("\nå„ç¨®æ’å€¼æ–¹æ³•çš„è©•ä¼°:")
print("-" * 50)
print(f"{'æ–¹æ³•':<20} {'å‡æ–¹èª¤å·® (MSE)':<15} {'å„ªç¼ºé»'}")
print("-" * 50)

results = {}
for name, method in [('å‰å‘å¡«å……', 'ffill'), ('å¾Œå‘å¡«å……', 'bfill'), 
                     ('ç·šæ€§æ’å€¼', 'linear'), ('å¤šé …å¼æ’å€¼', 'polynomial'),
                     ('æ™‚é–“æ’å€¼', 'time'), ('æ¨£æ¢æ’å€¼', 'spline')]:
    
    # éƒ¨åˆ†æ–¹æ³•éœ€è¦é¡å¤–åƒæ•¸
    if method in ['polynomial', 'spline']:
        filled = evaluation_ts.interpolate(method=method, order=3)
    else:
        filled = evaluation_ts.interpolate(method=method)
    
    # è¨ˆç®—å‡æ–¹èª¤å·®
    mse = ((filled[missing_idx] - complete_ts[missing_idx]) ** 2).mean()
    results[name] = mse
    
    # å„ªç¼ºé»
    if method == 'ffill':
        pros_cons = "å„ª: ç°¡å–®å¿«é€Ÿ, é©åˆéšæ¢¯ç‹€æ•¸æ“š; ç¼º: é•·æœŸç¼ºå¤±æ™‚ä¸æº–ç¢º"
    elif method == 'bfill':
        pros_cons = "å„ª: ç°¡å–®, é©åˆæœªä¾†å·²çŸ¥çš„æƒ…æ³; ç¼º: å¯èƒ½å¼•å…¥æœªä¾†ä¿¡æ¯"
    elif method == 'linear':
        pros_cons = "å„ª: ç°¡å–®ç›´è§€, è¨ˆç®—é«˜æ•ˆ; ç¼º: ç„¡æ³•æ•æ‰éç·šæ€§è¶¨å‹¢"
    elif method == 'polynomial':
        pros_cons = "å„ª: å¯æ•æ‰æ›²ç·šè¶¨å‹¢; ç¼º: éåº¦æ“¬åˆé¢¨éšª, é‚Šç·£æ•ˆæ‡‰"
    elif method == 'time':
        pros_cons = "å„ª: è€ƒæ…®æ™‚é–“é–“éš”, é©åˆä¸è¦å‰‡æ¡æ¨£; ç¼º: åƒ…é©ç”¨æ–¼æ™‚é–“ç´¢å¼•"
    elif method == 'spline':
        pros_cons = "å„ª: é«˜åº¦å¹³æ»‘, é©åˆé€£çºŒè®ŠåŒ–æ•¸æ“š; ç¼º: è¨ˆç®—è¤‡é›œ, å¯èƒ½éåº¦æ“¬åˆ"
    
    print(f"{name:<20} {mse:<15.4f} {pros_cons}")

# æ‰¾å‡ºæœ€ä½³æ–¹æ³•
best_method = min(results, key=results.get)
print(f"\nå°æ–¼æ­¤æ•¸æ“šé›†, æœ€ä½³æ’å€¼æ–¹æ³•ç‚º: {best_method} (MSE: {results[best_method]:.4f})")

# %% [markdown]
# ## ğŸ“Š 3. é€²éšé‡æ¡æ¨£èˆ‡é »ç‡è½‰æ›

# %%
# å‰µå»ºä¸€å€‹é«˜é »æ™‚é–“åºåˆ— - æ¯åˆ†é˜çš„ç¶²ç«™è¨ªå•è¨˜éŒ„
minute_data = pd.DataFrame({
    'timestamp': pd.date_range('2023-01-01', periods=60*24, freq='T'),  # ä¸€å¤©çš„æ¯åˆ†é˜æ•¸æ“š
    'visits': np.random.poisson(lam=5, size=60*24)  # æ³Šæ¾åˆ†å¸ƒæ¨¡æ“¬è¨ªå•æ¬¡æ•¸
})
minute_data.set_index('timestamp', inplace=True)

print("æ¯åˆ†é˜ç¶²ç«™è¨ªå•æ•¸æ“š (æ¨£æœ¬):")
print(minute_data.head())
print(f"æ•¸æ“šé»ç¸½æ•¸: {len(minute_data)}")

# %% [markdown]
# ### 3.1 é«˜ç´šé‡æ¡æ¨£æŠ€è¡“

# %%
# å®šç¾©å„ç¨®æ™‚é–“çª—å£çš„é‡æ¡æ¨£
resample_rules = {
    '5åˆ†é˜': '5T',
    'å°æ™‚': 'H',
    'æ—¥é–“ (9é»-17é»)': 'D',
    'å·¥ä½œæ—¥': 'B',
    'é€±': 'W',
    'æœˆ': 'M'
}

# ä¸åŒèšåˆæ–¹æ³•çš„æ¯”è¼ƒ
agg_methods = ['sum', 'mean', 'median', 'min', 'max']

# é¸æ“‡ä¸€å°æ™‚çš„æ•¸æ“šé€²è¡Œè©³ç´°å±•ç¤º
hourly_slice = minute_data['2023-01-01 09:00:00':'2023-01-01 09:59:59']

plt.figure(figsize=(12, 6))
plt.step(hourly_slice.index, hourly_slice['visits'], where='post', label='åŸå§‹æ¯åˆ†é˜æ•¸æ“š')

# æ·»åŠ 5åˆ†é˜é‡æ¡æ¨£
five_min_resampled = hourly_slice.resample('5T').sum()
plt.step(five_min_resampled.index, five_min_resampled['visits'], where='post', linewidth=2, label='5åˆ†é˜é‡æ¡æ¨£ (åˆè¨ˆ)')

plt.title('åŸå§‹æ¯åˆ†é˜æ•¸æ“švs 5åˆ†é˜é‡æ¡æ¨£')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# é¡¯ç¤ºä¸åŒé‡æ¡æ¨£è¦å‰‡å’Œèšåˆæ–¹æ³•çš„çµæœ
results = {}
for rule_name, rule in resample_rules.items():
    for method in agg_methods:
        result = minute_data.resample(rule).agg(method)
        results[f"{rule_name} ({method})"] = result

# ç¹ªè£½æ¯æ—¥ç¸½è¨ªå•é‡
daily_visits = minute_data.resample('D').sum()
plt.figure(figsize=(12, 6))
plt.bar(daily_visits.index, daily_visits['visits'], width=0.7)
plt.title('æ¯æ—¥ç¶²ç«™è¨ªå•ç¸½é‡')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('è¨ªå•æ¬¡æ•¸')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 3.2 è‡ªå®šç¾©é‡æ¡æ¨£èˆ‡åˆ†çµ„

# %%
# å‰µå»ºå·¥ä½œæ™‚é–“èˆ‡éå·¥ä½œæ™‚é–“çš„è‡ªå®šç¾©åˆ†çµ„
def business_nonbusiness(dt):
    if dt.weekday() < 5:  # 0-4æ˜¯é€±ä¸€è‡³é€±äº”
        if 9 <= dt.hour < 17:  # 9é»è‡³17é»
            return 'å·¥ä½œæ™‚é–“'
    return 'éå·¥ä½œæ™‚é–“'

# æŒ‰è‡ªå®šç¾©åˆ†çµ„èšåˆ
custom_grouped = minute_data.groupby(business_nonbusiness).agg(['sum', 'mean'])
print("å·¥ä½œæ™‚é–“èˆ‡éå·¥ä½œæ™‚é–“çš„è¨ªå•çµ±è¨ˆ:")
print(custom_grouped)

# ä½¿ç”¨å‡½æ•¸é‡æ¡æ¨£æ¯æ—¥æ•¸æ“šä¸¦ç¹ªè£½è¶¨å‹¢
daily_pattern = minute_data.groupby(minute_data.index.hour).mean()

plt.figure(figsize=(12, 6))
plt.plot(daily_pattern.index, daily_pattern['visits'], 'o-', linewidth=2)
plt.title('24å°æ™‚è¨ªå•æ¨¡å¼ (æ¯å°æ™‚å¹³å‡)')
plt.xlabel('å°æ™‚')
plt.ylabel('å¹³å‡è¨ªå•æ¬¡æ•¸')
plt.xticks(range(0, 24, 2))
plt.grid(True)
plt.tight_layout()
plt.show()

# æ¯é€±æ¨¡å¼åˆ†æ
# ç‚ºç°¡åŒ–å±•ç¤ºï¼Œå‰µå»ºä¸€å€‹æ›´é•·æœŸçš„æ•¸æ“šé›†
weekly_data = pd.DataFrame({
    'timestamp': pd.date_range('2023-01-01', periods=60*24*14, freq='T'),  # å…©é€±çš„æ¯åˆ†é˜æ•¸æ“š
    'visits': np.random.poisson(lam=5, size=60*24*14)  # æ³Šæ¾åˆ†å¸ƒæ¨¡æ“¬è¨ªå•æ¬¡æ•¸
})
weekly_data.set_index('timestamp', inplace=True)

# æŒ‰å·¥ä½œæ—¥å’Œå°æ™‚åˆ†çµ„
weekly_pattern = weekly_data.groupby([weekly_data.index.weekday, weekly_data.index.hour]).mean()
weekly_pattern = weekly_pattern.reset_index()
weekly_pattern.columns = ['weekday', 'hour', 'visits']

# è½‰æ›ç‚ºç¶²æ ¼å½¢å¼ä»¥ä¾¿ç†±åœ–ç¹ªè£½
weekly_grid = weekly_pattern.pivot(index='hour', columns='weekday', values='visits')

# å‰µå»ºæ˜ŸæœŸæ¨™ç±¤
weekday_labels = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']

# ç¹ªè£½ç†±åœ–
plt.figure(figsize=(12, 8))
sns.heatmap(weekly_grid, cmap='YlOrRd', linewidths=0.5, 
           xticklabels=weekday_labels, yticklabels=range(0, 24))
plt.title('é€±å…§æ¯å°æ™‚è¨ªå•æ¨¡å¼')
plt.xlabel('æ˜ŸæœŸ')
plt.ylabel('å°æ™‚')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 3.3 æ™‚å€èª¿æ•´èˆ‡æ™‚é–“åç§»

# %%
# å‰µå»ºè·¨æ™‚å€çš„æ™‚é–“åºåˆ—
international_data = pd.DataFrame({
    'timestamp': pd.date_range('2023-01-01', periods=24, freq='H'),
    'utc_visitors': np.random.randint(100, 500, 24)
})
international_data.set_index('timestamp', inplace=True)

# æ·»åŠ æ™‚å€ä¿¡æ¯ (UTC)
international_data.index = international_data.index.tz_localize('UTC')
print("UTCæ™‚å€çš„è¨ªå®¢æ•¸æ“š:")
print(international_data.head())

# è½‰æ›ç‚ºä¸åŒæ™‚å€
international_data['taipei_visitors'] = international_data['utc_visitors'].shift(-8)  # å°åŒ—æ™‚é–“ UTC+8
international_data['ny_visitors'] = international_data['utc_visitors'].shift(5)  # ç´ç´„æ™‚é–“ UTC-5
international_data['london_visitors'] = international_data['utc_visitors'].shift(0)  # å€«æ•¦æ™‚é–“ UTCÂ±0

# æª¢æŸ¥ä¸åŒæ™‚å€çš„è¨ªå®¢æ•¸æ“š
print("\nå„æ™‚å€çš„è¨ªå®¢æ•¸æ“š:")
print(international_data.head(10))

# è¨ˆç®—å…¨çƒç¸½è¨ªå®¢æ•¸ (ä¸è€ƒæ…®ç¼ºå¤±å€¼)
international_data['global_visitors'] = international_data[['utc_visitors', 'taipei_visitors', 
                                                            'ny_visitors', 'london_visitors']].sum(axis=1, skipna=True)

# è¦–è¦ºåŒ–è·¨æ™‚å€æµé‡åˆ†ä½ˆ
plt.figure(figsize=(12, 6))
for col in ['taipei_visitors', 'ny_visitors', 'london_visitors']:
    plt.plot(international_data.index, international_data[col], label=col.split('_')[0])

plt.title('ä¸åŒæ™‚å€çš„ç¶²ç«™è¨ªå®¢æ•¸é‡')
plt.xlabel('UTCæ™‚é–“')
plt.ylabel('è¨ªå®¢æ•¸')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ä½¿ç”¨æ™‚é–“åç§»é‡å‰µå»ºç‰¹æ®Šé »ç‡
offsets_example = pd.DataFrame({
    'date': pd.date_range('2023-01-01', periods=20, freq='D')
})
offsets_example.set_index('date', inplace=True)

# æ·»åŠ å„ç¨®åç§»æ—¥æœŸ
offsets_example['next_business_day'] = offsets_example.index + pd.offsets.BDay()
offsets_example['next_month_end'] = offsets_example.index + pd.offsets.MonthEnd()
offsets_example['next_quarter_end'] = offsets_example.index + pd.offsets.QuarterEnd()
offsets_example['same_day_next_month'] = offsets_example.index + pd.DateOffset(months=1)

print("\nå„ç¨®æ™‚é–“åç§»ç¯„ä¾‹:")
print(offsets_example.head())

# %% [markdown]
# ### 3.4 ä¸è¦å‰‡æ™‚é–“åºåˆ—èˆ‡è¿‘ä¼¼é‡æ¡æ¨£

# %%
# å‰µå»ºä¸è¦å‰‡é–“éš”çš„æ™‚é–“åºåˆ—
np.random.seed(42)
n_points = 50
irregular_timestamps = pd.to_datetime('2023-01-01') + pd.to_timedelta(np.sort(np.random.uniform(0, 100, n_points)), unit='d')
irregular_values = np.random.normal(100, 15, n_points)
irregular_ts = pd.Series(irregular_values, index=irregular_timestamps)

print("ä¸è¦å‰‡é–“éš”çš„æ™‚é–“åºåˆ—:")
print(irregular_ts.head())

# æª¢æŸ¥æ™‚é–“é–“éš”
time_diffs = irregular_ts.index.to_series().diff().dt.total_seconds() / (60*60*24)  # è½‰æ›ç‚ºå¤©æ•¸
print("\næ™‚é–“é»ä¹‹é–“çš„é–“éš” (å¤©):")
print(time_diffs.describe())

# ç¹ªè£½ä¸è¦å‰‡æ™‚é–“åºåˆ—
plt.figure(figsize=(12, 6))
plt.plot(irregular_ts.index, irregular_ts.values, 'o-', markersize=6, linewidth=1)
plt.title('ä¸è¦å‰‡é–“éš”çš„æ™‚é–“åºåˆ—')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('å€¼')
plt.grid(True)
plt.tight_layout()
plt.show()

# ä½¿ç”¨å„ç¨®æ–¹æ³•é€²è¡Œé‡æ¡æ¨£
# 1. ä½¿ç”¨asfreq()ä¾†å®šç¾©å›ºå®šé »ç‡ (ä½¿ç”¨å‰å€¼å¡«å……)
regular_asfreq = irregular_ts.asfreq('D', method='ffill')

# 2. ä½¿ç”¨resample()é€²è¡Œé‡æ¡æ¨£
regular_resample = irregular_ts.resample('D').mean()  # ä½¿ç”¨æ¯æ—¥å¹³å‡å€¼

# 3. ä½¿ç”¨Pandasçš„è¿‘ä¼¼åˆä½µ
# å‰µå»ºè¦å‰‡é »ç‡çš„ç©ºæ™‚é–“åºåˆ—
regular_dates = pd.date_range(start=irregular_ts.index.min(), 
                             end=irregular_ts.index.max(), freq='D')
regular_df = pd.DataFrame(index=regular_dates)

# ä½¿ç”¨merge_asofé€²è¡Œè¿‘ä¼¼åˆä½µ
regular_merge_asof = pd.merge_asof(
    regular_df,
    irregular_ts.reset_index().rename(columns={'index': 'date', 0: 'value'}),
    left_index=True,
    right_on='date',
    direction='backward'  # ä½¿ç”¨ä¹‹å‰æœ€è¿‘çš„å€¼
)
regular_merge_asof = regular_merge_asof['value']

# æª¢æŸ¥è™•ç†çµæœ
resampling_methods = {
    'åŸå§‹ä¸è¦å‰‡æ•¸æ“š': irregular_ts,
    'asfreq æ–¹æ³• (å‰å€¼å¡«å……)': regular_asfreq,
    'resample æ–¹æ³• (å¹³å‡å€¼)': regular_resample,
    'merge_asof æ–¹æ³• (è¿‘ä¼¼åˆä½µ)': regular_merge_asof
}

# å¯è¦–åŒ–æ¯”è¼ƒä¸åŒé‡æ¡æ¨£æ–¹æ³•
plt.figure(figsize=(12, 8))
for i, (name, data) in enumerate(resampling_methods.items(), 1):
    plt.subplot(2, 2, i)
    if name == 'åŸå§‹ä¸è¦å‰‡æ•¸æ“š':
        plt.plot(data.index, data.values, 'o-', markersize=6)
    else:
        plt.plot(data.index, data.values, '-')
    plt.title(name)
    plt.xticks(rotation=45)
    plt.grid(True)
plt.tight_layout()
plt.show()

# %% [markdown]
# ## ğŸ“Š 4. é€²éšçª—å£è¨ˆç®—èˆ‡ç§»å‹•å‡½æ•¸

# %%
# å‰µå»ºæ¨¡æ“¬è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“š
stock_dates = pd.date_range(start='2022-01-01', periods=252, freq='B')  # ä¸€å¹´çš„äº¤æ˜“æ—¥
np.random.seed(42)
# éš¨æ©ŸéŠèµ°æ¨¡æ“¬è‚¡ç¥¨åƒ¹æ ¼
price_changes = np.random.normal(0.0005, 0.01, 252)
stock_prices = 100 * np.exp(np.cumsum(price_changes))

# å‰µå»ºPandas DataFrame
stock_df = pd.DataFrame({
    'Close': stock_prices,
    'Volume': np.random.randint(100000, 1000000, 252)
}, index=stock_dates)

# æ·»åŠ é–‹ç›¤ã€æœ€é«˜ã€æœ€ä½åƒ¹æ ¼
stock_df['Open'] = stock_df['Close'].shift(1) * (1 + np.random.normal(0, 0.002, 252))
stock_df['High'] = np.maximum(stock_df['Close'], stock_df['Open']) * (1 + np.abs(np.random.normal(0, 0.003, 252)))
stock_df['Low'] = np.minimum(stock_df['Close'], stock_df['Open']) * (1 - np.abs(np.random.normal(0, 0.003, 252)))
stock_df = stock_df[['Open', 'High', 'Low', 'Close', 'Volume']]
stock_df = stock_df.fillna(method='bfill')

print("è‚¡ç¥¨OHLCVæ•¸æ“šæ¨£æœ¬:")
print(stock_df.head())

# ç¹ªè£½è‚¡åƒ¹èµ°å‹¢
plt.figure(figsize=(12, 6))
plt.plot(stock_df.index, stock_df['Close'])
plt.title('æ¨¡æ“¬è‚¡ç¥¨åƒ¹æ ¼èµ°å‹¢ (2022å¹´)')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æ”¶ç›¤åƒ¹')
plt.grid(True)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 4.1 è‡ªå®šç¾©çª—å£å‡½æ•¸

# %%
# å®šç¾©ä¸€å€‹è‡ªå®šç¾©çš„çª—å£å‡½æ•¸ä¾†è¨ˆç®—æ³¢å‹•ç‡
def volatility(x):
    """è¨ˆç®—åƒ¹æ ¼åºåˆ—çš„æ³¢å‹•ç‡ (ç§»å‹•æ¨™æº–å·®é™¤ä»¥å‡å€¼)"""
    return np.std(x) / np.mean(x) * 100  # çµæœä»¥ç™¾åˆ†æ¯”è¡¨ç¤º

# å®šç¾©ä¸€å€‹å‡½æ•¸è¨ˆç®—å¸ƒæ—å¸¶
def bollinger_bands(x, window=20, num_std=2):
    """è¨ˆç®—å¸ƒæ—å¸¶ (å‡å€¼Â±æ¨™æº–å·®)"""
    mean = x.mean()
    std = x.std()
    upper = mean + num_std * std
    lower = mean - num_std * std
    return pd.Series([lower, mean, upper], index=['lower', 'mean', 'upper'])

# å®šç¾©ä¸€å€‹å‡½æ•¸è¨ˆç®—ç›¸å°å¼·å¼±æŒ‡æ¨™ (RSI)
def compute_rsi(prices, window=14):
    """è¨ˆç®—RSIæŒ‡æ¨™"""
    # è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–
    delta = prices.diff()
    
    # åˆ†é›¢ä¸Šæ¼²å’Œä¸‹è·Œ
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    
    # è¨ˆç®—å¹³å‡å€¼
    avg_gain = gain.rolling(window=window).mean()
    avg_loss = loss.rolling(window=window).mean()
    
    # è¨ˆç®—ç›¸å°å¼·åº¦
    rs = avg_gain / avg_loss
    
    # è¨ˆç®—RSI
    rsi = 100 - (100 / (1 + rs))
    return rsi

# æ‡‰ç”¨å„ç¨®çª—å£è¨ˆç®—
# 1. è¨ˆç®—æ¨™æº–ç§»å‹•å¹³å‡ç·š
stock_df['SMA_20'] = stock_df['Close'].rolling(window=20).mean()
stock_df['SMA_50'] = stock_df['Close'].rolling(window=50).mean()
stock_df['SMA_200'] = stock_df['Close'].rolling(window=200).mean()

# 2. è¨ˆç®—æ³¢å‹•ç‡
stock_df['Volatility_20'] = stock_df['Close'].rolling(window=20).apply(volatility)

# 3. è¨ˆç®—å¸ƒæ—å¸¶
bollinger = stock_df['Close'].rolling(window=20).apply(lambda x: bollinger_bands(x))
bollinger_df = pd.DataFrame({
    'lower': [x['lower'] for x in bollinger],
    'mean': [x['mean'] for x in bollinger],
    'upper': [x['upper'] for x in bollinger]
}, index=stock_df.index)

# 4. è¨ˆç®—RSI
stock_df['RSI_14'] = compute_rsi(stock_df['Close'], window=14)

# 5. æ‡‰ç”¨è‡ªå®šç¾©çª—å£å‡½æ•¸
def price_momentum(prices, window=20):
    """è¨ˆç®—åƒ¹æ ¼å‹•é‡ (ç•¶å‰åƒ¹æ ¼ç›¸å°æ–¼çª—å£æœŸåˆåƒ¹æ ¼çš„è®ŠåŒ–ç™¾åˆ†æ¯”)"""
    return (prices[-1] / prices[0] - 1) * 100

stock_df['Momentum_20'] = stock_df['Close'].rolling(window=20).apply(price_momentum)

# æ‰“å°çµæœ
print("\næ‡‰ç”¨å„ç¨®çª—å£è¨ˆç®—å¾Œçš„æ•¸æ“š:")
print(stock_df.tail())

# ç¹ªè£½è‚¡åƒ¹èˆ‡ç§»å‹•å¹³å‡ç·š
plt.figure(figsize=(12, 6))
plt.plot(stock_df.index, stock_df['Close'], label='æ”¶ç›¤åƒ¹')
plt.plot(stock_df.index, stock_df['SMA_20'], label='20æ—¥å‡ç·š', linestyle='--')
plt.plot(stock_df.index, stock_df['SMA_50'], label='50æ—¥å‡ç·š', linestyle='-.')
plt.plot(stock_df.index, stock_df['SMA_200'], label='200æ—¥å‡ç·š', linestyle=':')
plt.title('è‚¡åƒ¹èˆ‡ç§»å‹•å¹³å‡ç·š')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('åƒ¹æ ¼')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ç¹ªè£½å¸ƒæ—å¸¶
plt.figure(figsize=(12, 6))
plt.plot(stock_df.index, stock_df['Close'], label='æ”¶ç›¤åƒ¹')
plt.plot(bollinger_df.index, bollinger_df['mean'], label='20æ—¥å‡ç·š', linestyle='--')
plt.fill_between(bollinger_df.index, bollinger_df['lower'], bollinger_df['upper'], alpha=0.2, label='å¸ƒæ—å¸¶å€é–“')
plt.title('å¸ƒæ—å¸¶æŒ‡æ¨™ (Bollinger Bands)')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('åƒ¹æ ¼')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ç¹ªè£½RSI
plt.figure(figsize=(12, 6))
plt.plot(stock_df.index, stock_df['RSI_14'])
plt.axhline(y=70, color='r', linestyle='--', alpha=0.5)
plt.axhline(y=30, color='g', linestyle='--', alpha=0.5)
plt.fill_between(stock_df.index, stock_df['RSI_14'], 70, where=(stock_df['RSI_14'] >= 70), color='r', alpha=0.3)
plt.fill_between(stock_df.index, stock_df['RSI_14'], 30, where=(stock_df['RSI_14'] <= 30), color='g', alpha=0.3)
plt.title('ç›¸å°å¼·å¼±æŒ‡æ¨™ (RSI)')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('RSIå€¼')
plt.legend(['RSI-14', 'è¶…è²·å€åŸŸ (70+)', 'è¶…è³£å€åŸŸ (30-)'])
plt.grid(True)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 4.2 çª—å£å‡½æ•¸èˆ‡åˆ†çµ„èšåˆçš„çµåˆ

# %%
# å‰µå»ºè·¨å¸‚å ´çš„è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“š
markets = ['ç¾åœ‹', 'å°ç£', 'æ—¥æœ¬', 'æ­æ´²']
stocks_per_market = 5
n_days = 252

multi_market_stocks = pd.DataFrame()

# ç‚ºæ¯å€‹å¸‚å ´å‰µå»ºè‚¡ç¥¨æ•¸æ“š
for market in markets:
    for i in range(1, stocks_per_market + 1):
        # ç‚ºä¸åŒå¸‚å ´è¨­å®šä¸åŒçš„æ³¢å‹•ç‰¹æ€§
        if market == 'ç¾åœ‹':
            mean_return = 0.0007
            volatility = 0.015
        elif market == 'å°ç£':
            mean_return = 0.0006
            volatility = 0.018
        elif market == 'æ—¥æœ¬':
            mean_return = 0.0004
            volatility = 0.014
        else:  # æ­æ´²
            mean_return = 0.0005
            volatility = 0.016
        
        # ç”Ÿæˆåƒ¹æ ¼åºåˆ—
        np.random.seed(42 + markets.index(market) * 10 + i)
        price_changes = np.random.normal(mean_return, volatility, n_days)
        prices = 100 * np.exp(np.cumsum(price_changes))
        
        # æ·»åŠ åˆ°DataFrame
        stock_id = f"{market}_{i}"
        temp_df = pd.DataFrame({
            'date': pd.date_range(start='2022-01-01', periods=n_days, freq='B'),
            'stock_id': stock_id,
            'market': market,
            'price': prices
        })
        multi_market_stocks = pd.concat([multi_market_stocks, temp_df])

# è¨­ç½®ç´¢å¼•
multi_market_stocks.set_index('date', inplace=True)

print("å¤šå¸‚å ´è‚¡ç¥¨åƒ¹æ ¼è³‡æ–™:")
print(multi_market_stocks.head())
print(f"è‚¡ç¥¨ç¸½æ•¸: {stocks_per_market * len(markets)}")
print(f"æ•¸æ“šç¸½è¡Œæ•¸: {len(multi_market_stocks)}")

# æŒ‰å¸‚å ´åˆ†çµ„è¨ˆç®—æ¯æ—¥å¹³å‡åƒ¹æ ¼
market_avg = multi_market_stocks.groupby(['market', multi_market_stocks.index])['price'].mean().unstack(level=0)

# ç¹ªè£½æ¯å€‹å¸‚å ´çš„å¹³å‡åƒ¹æ ¼è¶¨å‹¢
plt.figure(figsize=(12, 6))
for market in markets:
    plt.plot(market_avg.index, market_avg[market], label=market)
plt.title('å„å¸‚å ´å¹³å‡è‚¡ç¥¨åƒ¹æ ¼è¶¨å‹¢')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('å¹³å‡åƒ¹æ ¼')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# çµåˆåˆ†çµ„å’Œçª—å£è¨ˆç®—
# 1. è¨ˆç®—æ¯å€‹å¸‚å ´çš„20æ—¥æ»¾å‹•æ”¶ç›Šç‡
market_returns = market_avg.pct_change().rolling(20).mean() * 100
market_returns = market_returns.dropna()

# ç¹ªè£½20æ—¥ç§»å‹•å¹³å‡æ”¶ç›Šç‡
plt.figure(figsize=(12, 6))
for market in markets:
    plt.plot(market_returns.index, market_returns[market], label=market)
plt.title('å„å¸‚å ´20æ—¥å¹³å‡æ”¶ç›Šç‡')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æ”¶ç›Šç‡ (%)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. è¨ˆç®—æ¯å€‹è‚¡ç¥¨çš„æ³¢å‹•ç‡ï¼Œç„¶å¾ŒæŒ‰å¸‚å ´åˆ†çµ„æ¯”è¼ƒ
stock_volatility = multi_market_stocks.groupby('stock_id')['price'].apply(
    lambda x: x.pct_change().rolling(20).std().mean() * 100
)
stock_volatility = stock_volatility.reset_index()
stock_volatility['market'] = stock_volatility['stock_id'].str.split('_').str[0]

# æŒ‰å¸‚å ´åˆ†çµ„çš„æ³¢å‹•ç‡ç®±å‹åœ–
plt.figure(figsize=(12, 6))
sns.boxplot(x='market', y='price', data=stock_volatility)
plt.title('å„å¸‚å ´è‚¡ç¥¨æ³¢å‹•ç‡åˆ†å¸ƒ')
plt.xlabel('å¸‚å ´')
plt.ylabel('20æ—¥æ»¾å‹•æ³¢å‹•ç‡ (%)')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

# 3. è‡ªå®šç¾©åˆ†æï¼šè¨ˆç®—è·¨å¸‚å ´ç›¸é—œæ€§
# æ¯å€‹å¸‚å ´é¸æ“‡ç¬¬ä¸€æ”¯è‚¡ç¥¨è¨ˆç®—ç›¸é—œä¿‚æ•¸
selected_stocks = [f"{market}_1" for market in markets]
selected_data = multi_market_stocks[multi_market_stocks['stock_id'].isin(selected_stocks)]
pivot_data = selected_data.pivot(columns='stock_id', values='price')

# è¨ˆç®—æ”¶ç›Šç‡
returns = pivot_data.pct_change().dropna()

# è¨ˆç®—ä¸¦ç¹ªè£½ç›¸é—œæ€§ç†±åœ–
plt.figure(figsize=(10, 8))
corr_matrix = returns.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('å„å¸‚å ´è‚¡ç¥¨æ”¶ç›Šç‡ç›¸é—œæ€§')
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 4.3 æ»¾å‹•ç›¸é—œæ€§èˆ‡å”æ–¹å·®

# %%
# é¸æ“‡å…©å€‹å¸‚å ´é€²è¡Œæ·±å…¥åˆ†æ
market1 = 'ç¾åœ‹'
market2 = 'å°ç£'
stock1 = f"{market1}_1"
stock2 = f"{market2}_1"

# æå–é€™å…©å€‹è‚¡ç¥¨çš„æ•¸æ“š
paired_data = multi_market_stocks[multi_market_stocks['stock_id'].isin([stock1, stock2])]
pivot_paired = paired_data.pivot(columns='stock_id', values='price')

# è¨ˆç®—æ”¶ç›Šç‡
paired_returns = pivot_paired.pct_change().dropna()

# è¨ˆç®—60æ—¥æ»¾å‹•ç›¸é—œæ€§
rolling_corr = paired_returns[stock1].rolling(60).corr(paired_returns[stock2])

# ç¹ªè£½æ»¾å‹•ç›¸é—œæ€§
plt.figure(figsize=(12, 6))
plt.plot(rolling_corr.index, rolling_corr)
plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)
plt.title(f'{market1}èˆ‡{market2}å¸‚å ´è‚¡ç¥¨60æ—¥æ»¾å‹•ç›¸é—œæ€§')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('ç›¸é—œä¿‚æ•¸')
plt.grid(True)
plt.tight_layout()
plt.show()

# è¨ˆç®—60æ—¥æ»¾å‹•å”æ–¹å·®
rolling_cov = paired_returns[stock1].rolling(60).cov(paired_returns[stock2])

# ç¹ªè£½æ»¾å‹•å”æ–¹å·®
plt.figure(figsize=(12, 6))
plt.plot(rolling_cov.index, rolling_cov)
plt.title(f'{market1}èˆ‡{market2}å¸‚å ´è‚¡ç¥¨60æ—¥æ»¾å‹•å”æ–¹å·®')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('å”æ–¹å·®')
plt.grid(True)
plt.tight_layout()
plt.show()

# ç¶œåˆåˆ†æï¼šå‰µå»ºä¸€å€‹å¤šæŒ‡æ¨™åœ–è¡¨
fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)

# ç¹ªè£½è‚¡ç¥¨åƒ¹æ ¼ï¼ˆæ­¸ä¸€åŒ–ä»¥ä¾¿æ¯”è¼ƒï¼‰
normalized_prices = pivot_paired / pivot_paired.iloc[0]
normalized_prices.plot(ax=axes[0])
axes[0].set_title('æ­¸ä¸€åŒ–è‚¡ç¥¨åƒ¹æ ¼')
axes[0].set_ylabel('åƒ¹æ ¼ (åŸºæ–¼åˆå§‹å€¼)')
axes[0].legend([f'{market1}è‚¡ç¥¨', f'{market2}è‚¡ç¥¨'])
axes[0].grid(True)

# ç¹ªè£½æ»¾å‹•ç›¸é—œæ€§
rolling_corr.plot(ax=axes[1])
axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.3)
axes[1].set_title('60æ—¥æ»¾å‹•ç›¸é—œæ€§')
axes[1].set_ylabel('ç›¸é—œä¿‚æ•¸')
axes[1].grid(True)

# ç¹ªè£½æ»¾å‹•æ³¢å‹•ç‡
rolling_vol1 = paired_returns[stock1].rolling(20).std() * np.sqrt(252) * 100  # å¹´åŒ–æ³¢å‹•ç‡
rolling_vol2 = paired_returns[stock2].rolling(20).std() * np.sqrt(252) * 100
pd.DataFrame({stock1: rolling_vol1, stock2: rolling_vol2}).plot(ax=axes[2])
axes[2].set_title('20æ—¥æ»¾å‹•æ³¢å‹•ç‡ (å¹´åŒ–)')
axes[2].set_ylabel('æ³¢å‹•ç‡ (%)')
axes[2].legend([f'{market1}è‚¡ç¥¨', f'{market2}è‚¡ç¥¨'])
axes[2].grid(True)

plt.tight_layout()
plt.show()

# %% [markdown]
# ## ğŸ“Š 5. æ™‚é–“åºåˆ—åˆ†è§£èˆ‡è¶¨å‹¢åˆ†æ

# %% [markdown]
# ### 5.1 æ™‚é–“åºåˆ—çš„çµ„æˆæˆåˆ†

# %%
# å‰µå»ºä¸€å€‹å…·æœ‰æ˜é¡¯å­£ç¯€æ€§ã€è¶¨å‹¢å’Œå™ªè²çš„æ™‚é–“åºåˆ—
# ä½¿ç”¨ç¢ºå®šæ€§æ¨¡å‹ç”Ÿæˆæ•¸æ“š
np.random.seed(42)

# ç”Ÿæˆ4å¹´çš„æœˆåº¦æ•¸æ“š
dates = pd.date_range(start='2018-01-01', periods=48, freq='M')

# è¶¨å‹¢çµ„ä»¶: ç·šæ€§å¢é•·
trend = np.linspace(100, 150, 48)

# å­£ç¯€æ€§çµ„ä»¶: æ¯å¹´çš„å¾ªç’°æ¨¡å¼
seasonality = 15 * np.sin(np.arange(48) * (2 * np.pi / 12))

# å¾ªç’°çµ„ä»¶: è¼ƒé•·é€±æœŸçš„æ³¢å‹•
cycle = 10 * np.sin(np.arange(48) * (2 * np.pi / 24))

# å™ªè²çµ„ä»¶: éš¨æ©Ÿæ³¢å‹•
noise = np.random.normal(0, 5, 48)

# çµ„åˆæ‰€æœ‰çµ„ä»¶
ts_components = pd.DataFrame({
    'trend': trend,
    'seasonality': seasonality,
    'cycle': cycle,
    'noise': noise,
    'data': trend + seasonality + cycle + noise
}, index=dates)

print("æ™‚é–“åºåˆ—çµ„æˆæˆåˆ†:")
print(ts_components.head())

# ç¹ªè£½å„å€‹çµ„ä»¶
fig, axes = plt.subplots(5, 1, figsize=(12, 15), sharex=True)

# ç¹ªè£½åŸå§‹æ•¸æ“š
ts_components['data'].plot(ax=axes[0], title='åŸå§‹æ™‚é–“åºåˆ—æ•¸æ“š')
axes[0].set_ylabel('æ•¸å€¼')
axes[0].grid(True)

# ç¹ªè£½è¶¨å‹¢çµ„ä»¶
ts_components['trend'].plot(ax=axes[1], title='è¶¨å‹¢çµ„ä»¶')
axes[1].set_ylabel('æ•¸å€¼')
axes[1].grid(True)

# ç¹ªè£½å­£ç¯€æ€§çµ„ä»¶
ts_components['seasonality'].plot(ax=axes[2], title='å­£ç¯€æ€§çµ„ä»¶')
axes[2].set_ylabel('æ•¸å€¼')
axes[2].grid(True)

# ç¹ªè£½å¾ªç’°çµ„ä»¶
ts_components['cycle'].plot(ax=axes[3], title='å¾ªç’°çµ„ä»¶')
axes[3].set_ylabel('æ•¸å€¼')
axes[3].grid(True)

# ç¹ªè£½å™ªè²çµ„ä»¶
ts_components['noise'].plot(ax=axes[4], title='å™ªè²çµ„ä»¶')
axes[4].set_ylabel('æ•¸å€¼')
axes[4].grid(True)

plt.tight_layout()
plt.show()

# %% [markdown]
# ### 5.2 ç¶“å…¸æ™‚é–“åºåˆ—åˆ†è§£

# %%
# ä½¿ç”¨statsmodelsé€²è¡Œç¶“å…¸æ™‚é–“åºåˆ—åˆ†è§£
# åŠ æ³•æ¨¡å‹: Y(t) = T(t) + S(t) + e(t)
decomposition_add = seasonal_decompose(ts_components['data'], model='additive', period=12)

# ä¹˜æ³•æ¨¡å‹: Y(t) = T(t) * S(t) * e(t)
decomposition_mult = seasonal_decompose(ts_components['data'], model='multiplicative', period=12)

# ç¹ªè£½åŠ æ³•åˆ†è§£çµæœ
fig = plt.figure(figsize=(12, 10))
fig.suptitle('åŠ æ³•æ¨¡å‹æ™‚é–“åºåˆ—åˆ†è§£', fontsize=16)

plt.subplot(411)
plt.plot(ts_components.index, decomposition_add.observed)
plt.title('åŸå§‹æ™‚é–“åºåˆ—')
plt.grid(True)

plt.subplot(412)
plt.plot(ts_components.index, decomposition_add.trend)
plt.title('è¶¨å‹¢çµ„ä»¶')
plt.grid(True)

plt.subplot(413)
plt.plot(ts_components.index, decomposition_add.seasonal)
plt.title('å­£ç¯€æ€§çµ„ä»¶')
plt.grid(True)

plt.subplot(414)
plt.plot(ts_components.index, decomposition_add.resid)
plt.title('æ®˜å·® (å«å¾ªç’°èˆ‡å™ªè²)')
plt.grid(True)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

# ç¹ªè£½ä¹˜æ³•åˆ†è§£çµæœ
fig = plt.figure(figsize=(12, 10))
fig.suptitle('ä¹˜æ³•æ¨¡å‹æ™‚é–“åºåˆ—åˆ†è§£', fontsize=16)

plt.subplot(411)
plt.plot(ts_components.index, decomposition_mult.observed)
plt.title('åŸå§‹æ™‚é–“åºåˆ—')
plt.grid(True)

plt.subplot(412)
plt.plot(ts_components.index, decomposition_mult.trend)
plt.title('è¶¨å‹¢çµ„ä»¶')
plt.grid(True)

plt.subplot(413)
plt.plot(ts_components.index, decomposition_mult.seasonal)
plt.title('å­£ç¯€æ€§çµ„ä»¶')
plt.grid(True)

plt.subplot(414)
plt.plot(ts_components.index, decomposition_mult.resid)
plt.title('æ®˜å·® (å«å¾ªç’°èˆ‡å™ªè²)')
plt.grid(True)

plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

# %% [markdown]
# ### 5.3 é »ç‡åŸŸåˆ†æèˆ‡é€±æœŸè­˜åˆ¥

# %%
# å°åŸå§‹æ•¸æ“šé€²è¡Œå»è¶¨å‹¢å’Œå­£ç¯€æ€§è™•ç†
detrended = ts_components['data'] - decomposition_add.trend
deseasonalized = detrended - decomposition_add.seasonal

# ç¹ªè£½è™•ç†å¾Œçš„æ•¸æ“š
plt.figure(figsize=(12, 6))
plt.plot(ts_components.index, ts_components['data'], label='åŸå§‹æ•¸æ“š')
plt.plot(ts_components.index, detrended, label='å»é™¤è¶¨å‹¢')
plt.plot(ts_components.index, deseasonalized, label='å»é™¤è¶¨å‹¢å’Œå­£ç¯€æ€§')
plt.title('æ™‚é–“åºåˆ—æ•¸æ“šé è™•ç†')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æ•¸å€¼')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ä½¿ç”¨FFTåˆ†æé€±æœŸæ€§
from scipy import fftpack

# é è™•ç†: å»é™¤ç¼ºå¤±å€¼ä¸¦è½‰ç‚ºæ¨™æº–é–“éš”æ•¸æ“š
values = detrended.dropna().values
N = len(values)

# è¨ˆç®—FFT
f_values = fftpack.fft(values)
frequencies = fftpack.fftfreq(N, d=1)  # ä½¿ç”¨å–®ä½é–“éš”

# åªä¿ç•™æ­£é »ç‡
positive_frequencies = frequencies[1:N//2]
amplitudes = 2.0/N * np.abs(f_values[1:N//2])

# ç¹ªè£½æŒ¯å¹…è­œ
plt.figure(figsize=(12, 6))
plt.stem(positive_frequencies, amplitudes)
plt.title('é »ç‡åŸŸåˆ†æ (FFT)')
plt.xlabel('é »ç‡ (1/æœˆ)')
plt.ylabel('æŒ¯å¹…')
plt.grid(True)
plt.tight_layout()
plt.show()

# è­˜åˆ¥ä¸»è¦é€±æœŸæ€§
# é »ç‡è½‰æ›ç‚ºé€±æœŸ (æœˆæ•¸)
periods = 1 / positive_frequencies
# æ‰¾å‡ºå‰5å€‹ä¸»è¦é€±æœŸ
top_indices = np.argsort(amplitudes)[-5:]
top_periods = periods[top_indices]
top_amplitudes = amplitudes[top_indices]

print("\nè­˜åˆ¥å‡ºçš„ä¸»è¦é€±æœŸæ€§:")
for period, amplitude in zip(top_periods, top_amplitudes):
    if 1 <= period <= N/2:  # åªé¡¯ç¤ºåˆç†ç¯„åœå…§çš„é€±æœŸ
        if abs(period - 12) < 1:  # 12å€‹æœˆé€±æœŸ (å¹´åº¦)
            period_type = "å¹´åº¦é€±æœŸ"
        elif abs(period - 6) < 1:  # 6å€‹æœˆé€±æœŸ (åŠå¹´)
            period_type = "åŠå¹´åº¦é€±æœŸ"
        elif abs(period - 3) < 1:  # 3å€‹æœˆé€±æœŸ (å­£åº¦)
            period_type = "å­£åº¦é€±æœŸ"
        elif abs(period - 24) < 2:  # 24å€‹æœˆé€±æœŸ (å…©å¹´)
            period_type = "å…©å¹´åº¦é€±æœŸ"
        else:
            period_type = "å…¶ä»–é€±æœŸ"
        print(f"é€±æœŸ: {period:.2f} å€‹æœˆ (æŒ¯å¹…: {amplitude:.2f}) - å¯èƒ½æ˜¯{period_type}")

# %% [markdown]
# ### 5.4 æ™‚é–“åºåˆ—å¹³ç©©æ€§æª¢é©—

# %%
# ADFæª¢é©— (Augmented Dickey-Fuller test)
def check_stationarity(time_series, window=12, title=""):
    """
    åŸ·è¡Œæ™‚é–“åºåˆ—çš„å¹³ç©©æ€§æª¢é©—
    - ç¹ªè£½æ»¾å‹•å‡å€¼å’Œæ¨™æº–å·®
    - åŸ·è¡ŒADFæ¸¬è©¦
    """
    # ç¹ªè£½åŸå§‹æ•¸æ“šã€æ»¾å‹•å‡å€¼å’Œæ¨™æº–å·®
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.plot(time_series, label='åŸå§‹æ•¸æ“š')
    ax.plot(time_series.rolling(window=window).mean(), label=f'{window}å€‹æœˆæ»¾å‹•å¹³å‡')
    ax.plot(time_series.rolling(window=window).std(), label=f'{window}å€‹æœˆæ»¾å‹•æ¨™æº–å·®')
    ax.set_title(f'æ»¾å‹•çµ±è¨ˆé‡ - {title}')
    ax.set_xlabel('æ—¥æœŸ')
    ax.legend()
    ax.grid(True)
    
    # ADFæª¢é©—
    result = adfuller(time_series.dropna())
    print(f"ADFæ¸¬è©¦çµæœ - {title}:")
    print(f'ADFçµ±è¨ˆé‡: {result[0]:.4f}')
    print(f'på€¼: {result[1]:.4f}')
    print(f'æ»¯å¾Œé …æ•¸: {result[2]}')
    print(f'è§€æ¸¬å€¼æ•¸é‡: {result[3]}')
    for key, value in result[4].items():
        print(f'è‡¨ç•Œå€¼ ({key}): {value:.4f}')
    
    # åˆ¤æ–·æ˜¯å¦å¹³ç©©
    if result[1] <= 0.05:
        print("çµè«–: åºåˆ—å¯èƒ½æ˜¯å¹³ç©©çš„ (åœ¨5%é¡¯è‘—æ€§æ°´å¹³ä¸‹æ‹’çµ•å–®ä½æ ¹å­˜åœ¨çš„åŸå‡è¨­)")
    else:
        print("çµè«–: åºåˆ—å¯èƒ½æ˜¯éå¹³ç©©çš„ (ä¸èƒ½åœ¨5%é¡¯è‘—æ€§æ°´å¹³ä¸‹æ‹’çµ•å–®ä½æ ¹å­˜åœ¨çš„åŸå‡è¨­)")
    
    plt.tight_layout()
    plt.show()
    
    return result

# å°åŸå§‹æ•¸æ“šé€²è¡Œå¹³ç©©æ€§æª¢é©—
print("\nåŸå§‹æ™‚é–“åºåˆ—çš„å¹³ç©©æ€§æª¢é©—:")
original_adf = check_stationarity(ts_components['data'], title="åŸå§‹æ™‚é–“åºåˆ—")

# å°å»è¶¨å‹¢æ•¸æ“šé€²è¡Œå¹³ç©©æ€§æª¢é©—
print("\nå»è¶¨å‹¢æ™‚é–“åºåˆ—çš„å¹³ç©©æ€§æª¢é©—:")
detrended_adf = check_stationarity(detrended, title="å»è¶¨å‹¢æ™‚é–“åºåˆ—")

# å°å»é™¤è¶¨å‹¢å’Œå­£ç¯€æ€§çš„æ•¸æ“šé€²è¡Œå¹³ç©©æ€§æª¢é©—
print("\nå»é™¤è¶¨å‹¢å’Œå­£ç¯€æ€§å¾Œçš„æ™‚é–“åºåˆ—å¹³ç©©æ€§æª¢é©—:")
deseasonalized_adf = check_stationarity(deseasonalized, title="å»é™¤è¶¨å‹¢å’Œå­£ç¯€æ€§çš„æ™‚é–“åºåˆ—")

# å·®åˆ†æ³•ä½¿æ™‚é–“åºåˆ—å¹³ç©©
diff1 = ts_components['data'].diff().dropna()  # ä¸€éšå·®åˆ†
print("\nä¸€éšå·®åˆ†å¾Œçš„æ™‚é–“åºåˆ—å¹³ç©©æ€§æª¢é©—:")
diff1_adf = check_stationarity(diff1, title="ä¸€éšå·®åˆ†æ™‚é–“åºåˆ—")

# è‹¥ä¸€éšå·®åˆ†é‚„ä¸å¤ ï¼Œå¯ä»¥é€²è¡Œå­£ç¯€æ€§å·®åˆ†
seasonal_diff = ts_components['data'].diff(12).dropna()  # å­£ç¯€æ€§å·®åˆ† (12å€‹æœˆ)
print("\nå­£ç¯€æ€§å·®åˆ†å¾Œçš„æ™‚é–“åºåˆ—å¹³ç©©æ€§æª¢é©—:")
seasonal_diff_adf = check_stationarity(seasonal_diff, title="å­£ç¯€æ€§å·®åˆ†æ™‚é–“åºåˆ—")

# ç¹ªè£½ACFå’ŒPACFåœ– (è­˜åˆ¥ARIMAæ¨¡å‹çš„åƒæ•¸)
def plot_acf_pacf(series, lags=40, title=""):
    """ç¹ªè£½ACFå’ŒPACFåœ–"""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # ACF
    plot_acf(series.dropna(), lags=lags, ax=ax1)
    ax1.set_title(f'è‡ªç›¸é—œå‡½æ•¸ (ACF) - {title}')
    ax1.grid(True)
    
    # PACF
    plot_pacf(series.dropna(), lags=lags, ax=ax2)
    ax2.set_title(f'åè‡ªç›¸é—œå‡½æ•¸ (PACF) - {title}')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

# å°å»é™¤è¶¨å‹¢å’Œå­£ç¯€æ€§å¾Œçš„æ•¸æ“šç¹ªè£½ACFå’ŒPACF
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf_pacf(deseasonalized, title="å»é™¤è¶¨å‹¢å’Œå­£ç¯€æ€§çš„æ™‚é–“åºåˆ—") 

# %% [markdown]
# ## ğŸŒŸ 8. ç¸½çµèˆ‡é€²éšè³‡æº

# %%
# é¡¯ç¤ºæœ¬èª²ç¨‹çš„ç¸½çµå…§å®¹
print("ğŸ“š Pandas æ™‚é–“åºåˆ—åˆ†æèª²ç¨‹ç¸½çµ ğŸ“š")
print("=" * 50)
print("æœ¬èª²ç¨‹ä¸­ï¼Œæˆ‘å€‘æ·±å…¥å­¸ç¿’äº†ä»¥ä¸‹é€²éšæ™‚é–“åºåˆ—åˆ†ææŠ€è¡“ï¼š")
print("1. æ™‚é–“åºåˆ—æ•¸æ“šçµæ§‹èˆ‡åŸºæœ¬æ“ä½œ")
print("2. æ™‚é–“ç´¢å¼•èˆ‡æ—¥æœŸç¯„åœç”Ÿæˆ")
print("3. æ™‚å€è™•ç†èˆ‡è½‰æ›")
print("4. é‡æ¡æ¨£èˆ‡é »ç‡è½‰æ›")
print("5. æ»¾å‹•çª—å£è¨ˆç®—èˆ‡ç§»å‹•å¹³å‡")
print("6. è²¡å‹™æ™‚é–“åºåˆ—åˆ†æ")
print("7. å¯¦éš›æ¥­å‹™æ¡ˆä¾‹èˆ‡ç•°å¸¸æª¢æ¸¬")
print("=" * 50)
print("\né€²éšå­¸ç¿’è³‡æº:")
print("- å®˜æ–¹æ–‡ä»¶: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html")
print("- æ›¸ç±æ¨è–¦: 'Python for Data Analysis' by Wes McKinney")
print("- é€²éšèª²ç¨‹: 'Time Series Forecasting with Python' on Various Online Learning Platforms")
print("- ç›¸é—œå¥—ä»¶: statsmodels, prophet, pmdarima, tensorflow, pytorch")

# %% [markdown]
# ## ğŸ“Š 7. å¯¦éš›æ¥­å‹™æ¡ˆä¾‹èˆ‡ç•°å¸¸æª¢æ¸¬

# %% [markdown]
# ### 7.1 é›¶å”®éŠ·å”®é æ¸¬

# %%
# å‰µå»ºé›¶å”®éŠ·å”®æ•¸æ“š
np.random.seed(42)
date_range = pd.date_range(start='2018-01-01', end='2022-12-31', freq='D')
n_days = len(date_range)

# åŸºç¤éŠ·å”®é‡
base_sales = 1000

# å‰µå»ºå„ç¨®æ™‚é–“æ•ˆæ‡‰
# 1. å­£ç¯€æ€§ï¼šä¸€å¹´ç‚ºé€±æœŸ
annual_cycle = 200 * np.sin(np.arange(n_days) * (2 * np.pi / 365))

# 2. é€±æ•ˆæ‡‰ï¼šä¸€é€±ç‚ºé€±æœŸ (é€±æœ«éŠ·å”®é«˜)
weekday_effect = np.zeros(n_days)
for i, date in enumerate(date_range):
    if date.dayofweek >= 5:  # é€±æœ«
        weekday_effect[i] = 150
    elif date.dayofweek == 4:  # é€±äº”
        weekday_effect[i] = 100

# 3. ç¯€å‡æ—¥æ•ˆæ‡‰
holiday_effect = np.zeros(n_days)
for i, date in enumerate(date_range):
    # æ–°å¹´
    if date.month == 1 and date.day == 1:
        holiday_effect[i] = 300
    # è¾²æ›†æ–°å¹´ (ç°¡åŒ–ç‚ºå›ºå®š2æœˆåˆ)
    elif date.month == 2 and 1 <= date.day <= 5:
        holiday_effect[i] = 400
    # æ¸…æ˜ç¯€
    elif date.month == 4 and 4 <= date.day <= 6:
        holiday_effect[i] = 100
    # ç«¯åˆç¯€
    elif date.month == 6 and 20 <= date.day <= 22:
        holiday_effect[i] = 200
    # ä¸­ç§‹ç¯€
    elif date.month == 9 and 10 <= date.day <= 12:
        holiday_effect[i] = 300
    # é›™åç¯€
    elif date.month == 10 and date.day == 10:
        holiday_effect[i] = 200
    # è–èª•ç¯€/è·¨å¹´
    elif date.month == 12 and 20 <= date.day <= 31:
        holiday_effect[i] = 300 + (date.day - 20) * 20

# 4. é•·æœŸè¶¨å‹¢
trend = np.linspace(0, 500, n_days)  # 5å¹´å…§å¢åŠ 500çš„åŸºç¤éŠ·é‡

# 5. ç‰¹æ®Šäº‹ä»¶ (å¦‚ä¿ƒéŠ·æ´»å‹•)
promotion_effect = np.zeros(n_days)
# æ¯å­£åº¦ä¿ƒéŠ·
for year in range(2018, 2023):
    for month in [3, 6, 9, 12]:
        # æ¯å­£åº¦æœ€å¾Œä¸€å€‹æœˆçš„1è™Ÿé–‹å§‹ï¼ŒæŒçºŒ10å¤©
        start_idx = date_range.get_indexer([pd.Timestamp(f"{year}-{month}-01")])[0]
        promotion_effect[start_idx:start_idx+10] = 300

# 6. éš¨æ©Ÿå™ªè²
noise = np.random.normal(0, 50, n_days)

# åˆä½µæ‰€æœ‰æ•ˆæ‡‰
sales = base_sales + trend + annual_cycle + weekday_effect + holiday_effect + promotion_effect + noise
sales = np.maximum(sales, 0)  # ç¢ºä¿éŠ·å”®é‡éè² 

# å‰µå»ºéŠ·å”®æ•¸æ“šæ¡†
retail_data = pd.DataFrame({
    'Date': date_range,
    'Sales': sales,
    'IsWeekend': [1 if date.dayofweek >= 5 else 0 for date in date_range],
    'DayOfWeek': [date.dayofweek for date in date_range],
    'Month': [date.month for date in date_range],
    'Year': [date.year for date in date_range],
    'DayOfMonth': [date.day for date in date_range],
    'IsHoliday': [1 if effect > 0 else 0 for effect in holiday_effect],
    'IsPromotion': [1 if effect > 0 else 0 for effect in promotion_effect]
})

# è¨­ç½®æ—¥æœŸç‚ºç´¢å¼•
retail_data.set_index('Date', inplace=True)

print("é›¶å”®éŠ·å”®æ•¸æ“šé è¦½:")
print(retail_data.head())
print(f"\næ•¸æ“šç¯„åœ: {retail_data.index.min().strftime('%Y-%m-%d')} è‡³ {retail_data.index.max().strftime('%Y-%m-%d')}")
print(f"æ•¸æ“šé»æ•¸é‡: {len(retail_data)}")

# ç¹ªè£½éŠ·å”®æ™‚é–“åºåˆ—
plt.figure(figsize=(12, 6))
plt.plot(retail_data.index, retail_data['Sales'])
plt.title('é›¶å”®éŠ·å”®é‡ (2018-2022)')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('éŠ·å”®é‡')
plt.grid(True)
plt.tight_layout()
plt.show()

# 1. åŸºæœ¬æ™‚é–“åºåˆ—åˆ†æ
# è¨ˆç®—æ»¾å‹•å¹³å‡å’Œæ¨™æº–å·®ä»¥è­˜åˆ¥è¶¨å‹¢å’Œæ³¢å‹•
retail_data['SalesMA30'] = retail_data['Sales'].rolling(window=30).mean()
retail_data['SalesMA90'] = retail_data['Sales'].rolling(window=90).mean()
retail_data['SalesSTD30'] = retail_data['Sales'].rolling(window=30).std()

# å¯è¦–åŒ–è¶¨å‹¢å’Œæ³¢å‹•
plt.figure(figsize=(12, 8))
# ç¹ªè£½éŠ·å”®å’Œç§»å‹•å¹³å‡
plt.subplot(2, 1, 1)
plt.plot(retail_data.index, retail_data['Sales'], alpha=0.6, label='æ¯æ—¥éŠ·å”®')
plt.plot(retail_data.index, retail_data['SalesMA30'], label='30æ—¥ç§»å‹•å¹³å‡')
plt.plot(retail_data.index, retail_data['SalesMA90'], label='90æ—¥ç§»å‹•å¹³å‡')
plt.title('é›¶å”®éŠ·å”®è¶¨å‹¢åˆ†æ')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('éŠ·å”®é‡')
plt.legend()
plt.grid(True)

# ç¹ªè£½æ³¢å‹•æ€§
plt.subplot(2, 1, 2)
plt.plot(retail_data.index, retail_data['SalesSTD30'], label='30æ—¥æ»¾å‹•æ¨™æº–å·®')
plt.title('éŠ·å”®æ³¢å‹•æ€§åˆ†æ')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æ¨™æº–å·®')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. å­£ç¯€æ€§åˆ†è§£
# ä½¿ç”¨STLåˆ†è§£æ³•å°‡æ™‚é–“åºåˆ—åˆ†è§£ç‚ºè¶¨å‹¢ã€å­£ç¯€æ€§å’Œæ®˜å·®
from statsmodels.tsa.seasonal import STL

# æ¯æ—¥æ•¸æ“šé‡æ¡æ¨£ç‚ºé€±æ•¸æ“šä»¥ç°¡åŒ–åˆ†æ
weekly_sales = retail_data['Sales'].resample('W').mean()

# STLåˆ†è§£ (é€±åº¦æ•¸æ“š)
stl = STL(weekly_sales, period=52)  # 52é€±ç‚ºä¸€å¹´
result = stl.fit()

# å¯è¦–åŒ–åˆ†è§£çµæœ
plt.figure(figsize=(12, 10))
plt.subplot(4, 1, 1)
plt.plot(result.observed)
plt.title('åŸå§‹é€±éŠ·å”®æ•¸æ“š')
plt.subplot(4, 1, 2)
plt.plot(result.trend)
plt.title('è¶¨å‹¢æˆåˆ†')
plt.subplot(4, 1, 3)
plt.plot(result.seasonal)
plt.title('å­£ç¯€æ€§æˆåˆ†')
plt.subplot(4, 1, 4)
plt.plot(result.resid)
plt.title('æ®˜å·®æˆåˆ†')
plt.tight_layout()
plt.show()

# 3. é€±åº¦å’Œæœˆåº¦æ¨¡å¼åˆ†æ
# é€±åº¦æ¨¡å¼
weekly_pattern = retail_data.groupby('DayOfWeek')['Sales'].mean()
weekly_pattern = weekly_pattern / weekly_pattern.sum()

# æœˆåº¦æ¨¡å¼
monthly_pattern = retail_data.groupby('Month')['Sales'].mean()
monthly_pattern = monthly_pattern / monthly_pattern.sum()

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.barplot(x=weekly_pattern.index, y=weekly_pattern.values)
plt.title('å¹³å‡æ¯é€±éŠ·å”®æ¨¡å¼')
plt.xlabel('æ˜ŸæœŸå¹¾ (0=é€±ä¸€, 6=é€±æ—¥)')
plt.ylabel('ç›¸å°éŠ·å”®æ¯”ä¾‹')

plt.subplot(1, 2, 2)
sns.barplot(x=monthly_pattern.index, y=monthly_pattern.values)
plt.title('å¹³å‡æ¯æœˆéŠ·å”®æ¨¡å¼')
plt.xlabel('æœˆä»½')
plt.ylabel('ç›¸å°éŠ·å”®æ¯”ä¾‹')
plt.tight_layout()
plt.show()

# 4. ç°¡å–®çš„éŠ·å”®é æ¸¬æ¨¡å‹
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error

# é¸æ“‡æœ€è¿‘ä¸€å¹´çš„æ•¸æ“šé€²è¡Œé æ¸¬è©•ä¼°
train_data = weekly_sales[:-52]
test_data = weekly_sales[-52:]

# å˜—è©¦åŸºæœ¬çš„SARIMAæ¨¡å‹ï¼Œè‡ªå‹•å°‹æ‰¾æœ€ä½³åƒæ•¸
model = SARIMAX(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))
results = model.fit(disp=False)

# ç”Ÿæˆé æ¸¬
forecast = results.forecast(steps=len(test_data))

# è©•ä¼°é æ¸¬æ€§èƒ½
mae = mean_absolute_error(test_data, forecast)
rmse = np.sqrt(mean_squared_error(test_data, forecast))
mape = np.mean(np.abs((test_data - forecast) / test_data)) * 100

print("\né æ¸¬è©•ä¼°æŒ‡æ¨™:")
print(f"å¹³å‡çµ•å°èª¤å·® (MAE): {mae:.2f}")
print(f"å‡æ–¹æ ¹èª¤å·® (RMSE): {rmse:.2f}")
print(f"å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·® (MAPE): {mape:.2f}%")

# ç¹ªè£½é æ¸¬çµæœ
plt.figure(figsize=(12, 6))
plt.plot(train_data.index, train_data, label='è¨“ç·´æ•¸æ“š')
plt.plot(test_data.index, test_data, label='å¯¦éš›éŠ·å”®')
plt.plot(test_data.index, forecast, label='é æ¸¬éŠ·å”®')
plt.title('éŠ·å”®é æ¸¬æ¨¡å‹è©•ä¼°')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('é€±éŠ·å”®é‡')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 5. ç¯€å‡æ—¥å’Œä¿ƒéŠ·æ•ˆæ‡‰åˆ†æ
# å‰µå»ºä¸€å€‹æ¨¡å‹ä¾†ä¼°è¨ˆå„ç¨®å› ç´ å°éŠ·å”®çš„å½±éŸ¿
import statsmodels.api as sm

# ä½¿ç”¨æ—¥æ•¸æ“šä»¥æ•æ‰ç¯€å‡æ—¥æ•ˆæ‡‰
features = retail_data[['IsWeekend', 'IsHoliday', 'IsPromotion', 'Month', 'DayOfMonth']].copy()

# å°‡æœˆä»½å’Œæ˜ŸæœŸå¹¾è½‰æ›ç‚ºonehotç·¨ç¢¼
month_dummies = pd.get_dummies(retail_data['Month'], prefix='Month')
dow_dummies = pd.get_dummies(retail_data['DayOfWeek'], prefix='DoW')

# åˆä½µç‰¹å¾µ
X = pd.concat([features, month_dummies, dow_dummies], axis=1)
y = retail_data['Sales']

# æ·»åŠ å¸¸æ•¸é …
X = sm.add_constant(X)

# æ“¬åˆç·šæ€§æ¨¡å‹
model = sm.OLS(y, X).fit()
print("\néŠ·å”®å› ç´ åˆ†æ:")
print(model.summary().tables[1])

# æå–é‡è¦ä¿‚æ•¸
coefficients = model.params
significant = model.pvalues < 0.05
significant_coef = coefficients[significant]
top_features = significant_coef.abs().sort_values(ascending=False).head(10)

# å¯è¦–åŒ–æœ€é‡è¦çš„å½±éŸ¿å› ç´ 
plt.figure(figsize=(12, 6))
top_features.plot(kind='bar')
plt.title('éŠ·å”®çš„ä¸»è¦å½±éŸ¿å› ç´ ')
plt.xlabel('ç‰¹å¾µ')
plt.ylabel('å½±éŸ¿ä¿‚æ•¸')
plt.grid(True)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 7.2 ç¶²ç«™æµé‡ç•°å¸¸æª¢æ¸¬

# %%
# å‰µå»ºç¶²ç«™æµé‡æ•¸æ“š
np.random.seed(43)
date_range = pd.date_range(start='2022-01-01', end='2022-12-31', freq='H')
n_hours = len(date_range)

# åŸºç¤æµé‡
base_traffic = 1000

# å‰µå»ºå„ç¨®æ™‚é–“æ•ˆæ‡‰
# 1. æ—¥å…§è®ŠåŒ–ï¼šäººå€‘åœ¨ç™½å¤©ä¸Šç¶²è¼ƒå¤š
hourly_pattern = np.zeros(24)
for hour in range(24):
    if 0 <= hour < 6:  # å‡Œæ™¨
        hourly_pattern[hour] = 0.3
    elif 6 <= hour < 9:  # æ—©ä¸Šä¸Šç­å‰
        hourly_pattern[hour] = 0.8 + (hour - 6) * 0.2
    elif 9 <= hour < 12:  # ä¸Šåˆå·¥ä½œæ™‚é–“
        hourly_pattern[hour] = 1.2
    elif 12 <= hour < 14:  # åˆä¼‘
        hourly_pattern[hour] = 1.4
    elif 14 <= hour < 18:  # ä¸‹åˆå·¥ä½œæ™‚é–“
        hourly_pattern[hour] = 1.3
    elif 18 <= hour < 22:  # æ™šä¸Šä¼‘é–’æ™‚é–“
        hourly_pattern[hour] = 1.5
    else:  # æ·±å¤œ
        hourly_pattern[hour] = 0.7

daily_pattern = np.array([hourly_pattern[date.hour] for date in date_range])

# 2. é€±é–“è®ŠåŒ–ï¼šé€±æœ«æµé‡é«˜æ–¼å·¥ä½œæ—¥
weekly_pattern = np.zeros(n_hours)
for i, date in enumerate(date_range):
    if date.dayofweek >= 5:  # é€±æœ«
        weekly_pattern[i] = 1.2
    else:  # å·¥ä½œæ—¥
        weekly_pattern[i] = 1.0

# 3. é•·æœŸè¶¨å‹¢ï¼šæµé‡é€æ¼¸å¢åŠ 
trend = np.linspace(1, 1.3, n_hours)

# 4. ç‰¹æ®Šäº‹ä»¶ï¼šDDoSæ”»æ“Š(æµé‡ç•°å¸¸å°–å³°)ã€æœå‹™ä¸­æ–·(æµé‡é©Ÿé™)ã€ç‡ŸéŠ·æ´»å‹•(æµé‡ä¸Šå‡)
events = np.zeros(n_hours)

# DDoSæ”»æ“Š (3æ¬¡)
for month in [2, 5, 10]:
    # æ¯æ¬¡æŒçºŒ6å°æ™‚
    start_idx = date_range.get_indexer([pd.Timestamp(f"2022-{month}-15 10:00:00")])[0]
    events[start_idx:start_idx+6] = 10.0  # æµé‡æš´å¢10å€

# æœå‹™ä¸­æ–· (2æ¬¡)
for month in [4, 8]:
    # æ¯æ¬¡æŒçºŒ3å°æ™‚
    start_idx = date_range.get_indexer([pd.Timestamp(f"2022-{month}-10 14:00:00")])[0]
    events[start_idx:start_idx+3] = -0.9  # æµé‡é™è‡³æ­£å¸¸çš„10%

# ç‡ŸéŠ·æ´»å‹• (4æ¬¡)
for month in [3, 6, 9, 12]:
    # æ¯æ¬¡æŒçºŒ3å¤©
    start_idx = date_range.get_indexer([pd.Timestamp(f"2022-{month}-01 00:00:00")])[0]
    events[start_idx:start_idx+72] = 2.0  # æµé‡å¢åŠ 2å€

# 5. éš¨æ©Ÿå™ªè²
noise = np.random.normal(0, 0.1, n_hours)

# åˆä½µæ‰€æœ‰æ•ˆæ‡‰
traffic_multiplier = daily_pattern * weekly_pattern * trend * (1 + events)
traffic = base_traffic * traffic_multiplier * (1 + noise)
traffic = np.maximum(traffic, 0)  # ç¢ºä¿æµé‡éè² 

# å‰µå»ºç¶²ç«™æµé‡æ•¸æ“šæ¡†
traffic_data = pd.DataFrame({
    'Timestamp': date_range,
    'Traffic': traffic,
    'Hour': [date.hour for date in date_range],
    'DayOfWeek': [date.dayofweek for date in date_range],
    'Month': [date.month for date in date_range],
    'DayOfMonth': [date.day for date in date_range],
    'IsWeekend': [1 if date.dayofweek >= 5 else 0 for date in date_range]
})

# è¨­ç½®æ™‚é–“æˆ³ç‚ºç´¢å¼•
traffic_data.set_index('Timestamp', inplace=True)

print("ç¶²ç«™æµé‡æ•¸æ“šé è¦½:")
print(traffic_data.head())
print(f"\næ•¸æ“šç¯„åœ: {traffic_data.index.min().strftime('%Y-%m-%d %H:%M')} è‡³ {traffic_data.index.max().strftime('%Y-%m-%d %H:%M')}")
print(f"æ•¸æ“šé»æ•¸é‡: {len(traffic_data)}")

# ç¹ªè£½ä¸€å€‹æœˆçš„æµé‡æ™‚é–“åºåˆ—ä»¥ä¾¿è§€å¯Ÿæ¨¡å¼
start_date = '2022-03-01'
end_date = '2022-03-31'
monthly_traffic = traffic_data.loc[start_date:end_date]

plt.figure(figsize=(12, 6))
plt.plot(monthly_traffic.index, monthly_traffic['Traffic'])
plt.title(f'ç¶²ç«™æµé‡ ({start_date} è‡³ {end_date})')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æµé‡')
plt.grid(True)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 7.3 ç•°å¸¸æª¢æ¸¬æŠ€è¡“

# %%
# ä½¿ç”¨å¤šç¨®æ–¹æ³•æª¢æ¸¬ç¶²ç«™æµé‡ç•°å¸¸

# 1. åŸºæ–¼çµ±è¨ˆçš„æ–¹æ³• - Z-åˆ†æ•¸æ³•
def detect_anomalies_zscore(data, column, window=24*7, threshold=3):
    """ä½¿ç”¨ç§»å‹•çª—å£Z-åˆ†æ•¸æ³•æª¢æ¸¬ç•°å¸¸"""
    rolling_mean = data[column].rolling(window=window).mean()
    rolling_std = data[column].rolling(window=window).std()
    
    # è¨ˆç®—Zåˆ†æ•¸
    z_scores = (data[column] - rolling_mean) / rolling_std
    
    # è­˜åˆ¥ç•°å¸¸
    anomalies = data[abs(z_scores) > threshold].copy()
    anomalies['Z-Score'] = z_scores[abs(z_scores) > threshold]
    anomalies['Method'] = 'Z-Score'
    
    return anomalies

# 2. ç§»å‹•å¹³å‡æ³• - æª¢æ¸¬èˆ‡è¶¨å‹¢çš„åé›¢
def detect_anomalies_ma(data, column, window=24*7, threshold=3):
    """ä½¿ç”¨ç§»å‹•å¹³å‡èˆ‡æ¨™æº–å·®æª¢æ¸¬ç•°å¸¸"""
    rolling_mean = data[column].rolling(window=window).mean()
    rolling_std = data[column].rolling(window=window).std()
    
    # è¨ˆç®—ä¸Šä¸‹é–¾å€¼
    upper_bound = rolling_mean + threshold * rolling_std
    lower_bound = rolling_mean - threshold * rolling_std
    
    # è­˜åˆ¥ç•°å¸¸
    anomalies = data[(data[column] > upper_bound) | (data[column] < lower_bound)].copy()
    anomalies['UpperBound'] = upper_bound[(data[column] > upper_bound) | (data[column] < lower_bound)]
    anomalies['LowerBound'] = lower_bound[(data[column] > upper_bound) | (data[column] < lower_bound)]
    anomalies['Method'] = 'Moving Average'
    
    return anomalies

# 3. åŸºæ–¼IQRçš„æ–¹æ³•ï¼ˆç®±å½¢åœ–æ³•å‰‡ï¼‰
def detect_anomalies_iqr(data, column, window=24*7, threshold=1.5):
    """ä½¿ç”¨ç§»å‹•çª—å£IQRæª¢æ¸¬ç•°å¸¸"""
    results = []
    
    for i in range(window, len(data)):
        subset = data[column].iloc[i-window:i]
        q1 = subset.quantile(0.25)
        q3 = subset.quantile(0.75)
        iqr = q3 - q1
        
        lower_bound = q1 - threshold * iqr
        upper_bound = q3 + threshold * iqr
        
        current_value = data[column].iloc[i]
        
        if current_value > upper_bound or current_value < lower_bound:
            results.append({
                'Timestamp': data.index[i],
                column: current_value,
                'UpperBound': upper_bound,
                'LowerBound': lower_bound,
                'Method': 'IQR'
            })
    
    return pd.DataFrame(results).set_index('Timestamp') if results else pd.DataFrame()

# æ‡‰ç”¨ç•°å¸¸æª¢æ¸¬æ–¹æ³•
window_size = 24*7  # 7å¤©æ»¾å‹•çª—å£
z_anomalies = detect_anomalies_zscore(traffic_data, 'Traffic', window=window_size, threshold=3)
ma_anomalies = detect_anomalies_ma(traffic_data, 'Traffic', window=window_size, threshold=3)
iqr_anomalies = detect_anomalies_iqr(traffic_data, 'Traffic', window=window_size, threshold=1.5)

# åˆä½µçµæœ
all_anomalies = pd.concat([z_anomalies, ma_anomalies, iqr_anomalies])
all_anomalies = all_anomalies[~all_anomalies.index.duplicated(keep='first')]
all_anomalies.sort_index(inplace=True)

print(f"æª¢æ¸¬åˆ° {len(all_anomalies)} å€‹ç•°å¸¸é»")
print("ç•°å¸¸é»åˆ†å¸ƒ:")
print(all_anomalies.groupby('Method').size())

# å¯è¦–åŒ–éƒ¨åˆ†æ™‚é–“ç¯„åœçš„ç•°å¸¸æª¢æ¸¬çµæœ
# é¸æ“‡å…·æœ‰ç•°å¸¸çš„æ™‚é–“ç¯„åœé€²è¡Œå¯è¦–åŒ–
start_date = '2022-05-01'
end_date = '2022-05-31'

# ç¯©é¸è©²æ™‚é–“ç¯„åœçš„æ•¸æ“š
period_data = traffic_data.loc[start_date:end_date]
period_anomalies = all_anomalies.loc[start_date:end_date]

# è¨ˆç®—è©²æœŸé–“çš„ç§»å‹•å¹³å‡å’Œæ¨™æº–å·®é‚Šç•Œ
rolling_mean = period_data['Traffic'].rolling(window=window_size).mean()
rolling_std = period_data['Traffic'].rolling(window=window_size).std()
upper_bound = rolling_mean + 3 * rolling_std
lower_bound = rolling_mean - 3 * rolling_std

# ç¹ªè£½åŸå§‹æ•¸æ“šã€ç§»å‹•å¹³å‡ç·šå’Œç•°å¸¸é»
plt.figure(figsize=(15, 8))
plt.plot(period_data.index, period_data['Traffic'], label='æµé‡', alpha=0.7)
plt.plot(rolling_mean.loc[start_date:end_date].index, 
         rolling_mean.loc[start_date:end_date], 
         'g--', label='ç§»å‹•å¹³å‡')
plt.plot(upper_bound.loc[start_date:end_date].index, 
         upper_bound.loc[start_date:end_date], 
         'r--', label='ä¸Šç•Œ (3Ïƒ)')
plt.plot(lower_bound.loc[start_date:end_date].index, 
         lower_bound.loc[start_date:end_date], 
         'r--', label='ä¸‹ç•Œ (3Ïƒ)')

# æ¨™è¨˜ä¸åŒé¡å‹çš„ç•°å¸¸
for method in period_anomalies['Method'].unique():
    method_anomalies = period_anomalies[period_anomalies['Method'] == method]
    plt.scatter(method_anomalies.index, method_anomalies['Traffic'], 
               label=f'ç•°å¸¸ ({method})', s=50, alpha=0.8)

plt.title(f'ç¶²ç«™æµé‡ç•°å¸¸æª¢æ¸¬ ({start_date} è‡³ {end_date})')
plt.xlabel('æ—¥æœŸ')
plt.ylabel('æµé‡')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ç•°å¸¸åˆ†é¡å’Œåˆ†æ
# å°‡ç•°å¸¸åˆ†é¡ç‚ºï¼šçªå¢ã€çªé™å’Œç•°å¸¸æ³¢å‹•
def classify_anomalies(anomalies, data, window=24):
    """å°ç•°å¸¸é€²è¡Œåˆ†é¡"""
    classified = anomalies.copy()
    classified['AnomalyType'] = 'Unknown'
    
    for idx in classified.index:
        # ç²å–ç•¶å‰å€¼å’Œå‰ä¸€æ®µæ™‚é–“çš„å¹³å‡å€¼
        current_value = data.loc[idx, 'Traffic']
        if idx - pd.Timedelta(hours=window) in data.index:
            prev_window = data.loc[idx - pd.Timedelta(hours=window):idx - pd.Timedelta(hours=1)]
            if len(prev_window) > 0:
                prev_mean = prev_window['Traffic'].mean()
                percent_change = (current_value - prev_mean) / prev_mean
                
                # åˆ†é¡ç•°å¸¸
                if percent_change > 1.0:  # æµé‡å¢åŠ è¶…é100%
                    classified.loc[idx, 'AnomalyType'] = 'Spike'
                elif percent_change < -0.5:  # æµé‡æ¸›å°‘è¶…é50%
                    classified.loc[idx, 'AnomalyType'] = 'Drop'
                else:
                    classified.loc[idx, 'AnomalyType'] = 'Fluctuation'
    
    return classified

# å°ç•°å¸¸é€²è¡Œåˆ†é¡
classified_anomalies = classify_anomalies(all_anomalies, traffic_data)

# æŸ¥çœ‹ç•°å¸¸é¡å‹åˆ†å¸ƒ
anomaly_counts = classified_anomalies.groupby('AnomalyType').size()
print("\nç•°å¸¸é¡å‹åˆ†å¸ƒ:")
print(anomaly_counts)

# ç¹ªè£½ç•°å¸¸é¡å‹åˆ†å¸ƒ
plt.figure(figsize=(10, 6))
anomaly_counts.plot(kind='bar')
plt.title('ç¶²ç«™æµé‡ç•°å¸¸é¡å‹åˆ†å¸ƒ')
plt.xlabel('ç•°å¸¸é¡å‹')
plt.ylabel('æ•¸é‡')
plt.grid(True)
plt.tight_layout()
plt.show()

# æ ¹æ“šæ™‚é–“åˆ†å¸ƒåˆ†æç•°å¸¸
# æŒ‰å°æ™‚åˆ†æç•°å¸¸
hourly_anomalies = classified_anomalies.groupby(classified_anomalies.index.hour).size()
# æŒ‰æ˜ŸæœŸå¹¾åˆ†æç•°å¸¸
weekly_anomalies = classified_anomalies.groupby(classified_anomalies.index.dayofweek).size()
# æŒ‰æœˆåˆ†æç•°å¸¸
monthly_anomalies = classified_anomalies.groupby(classified_anomalies.index.month).size()

plt.figure(figsize=(15, 10))
plt.subplot(3, 1, 1)
hourly_anomalies.plot(kind='bar')
plt.title('ç•°å¸¸æŒ‰å°æ™‚åˆ†å¸ƒ')
plt.xlabel('å°æ™‚')
plt.ylabel('ç•°å¸¸æ•¸é‡')
plt.grid(True)

plt.subplot(3, 1, 2)
weekly_anomalies.plot(kind='bar')
plt.title('ç•°å¸¸æŒ‰æ˜ŸæœŸå¹¾åˆ†å¸ƒ')