{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA \n",
    "- goal is not to produce nice graphics and basic stats to win kernal votes\n",
    "- goal is to uncover patterns that can lead to new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. goal is to have complete code to produce a submission quickly\n",
    "2. build a simple model: eg, xgboost\n",
    "3. then progressively try more complex models (NN, stacking) and try adding engineered features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross vaildation setting\n",
    "\n",
    "- goal: be able to know if a new model is going to improve our private LB score\n",
    "\n",
    "## wrong way\n",
    "submit, and see it the public LB score improves\n",
    "\n",
    "## right way\n",
    "- use the train data to vaildate model or feature engineering\n",
    "- K fold cross vaildation (k b/w 3 to 10)\n",
    "time based cross vaildation if time dependent data\n",
    "- train on all train periods except last, and predict on last\n",
    "- then retrain on all data and submit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "always look at the gap between train and val score, a large gap is an indication of overfitting\n",
    "\n",
    "when you add a feature, check that it does not widden the tain/cv gap a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering: try a lot and fail fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try usual suspects\n",
    "- Counts\n",
    "- Target Encoding\n",
    "- Clustering\n",
    "- sin(x), cos(x) for periodic feature hours, minutes, days\n",
    "- lag variables for time series\n",
    "- No missing value imputation, no OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune algorithm/ model parameters\n",
    "Impossiable without reliable validation setting\n",
    "\n",
    "\n",
    "Don't overtune your parameters: do it once, maybe twice in competition, no more.\n",
    "\n",
    "For XGboost/ LightGBM\n",
    "- start with subsample = 0.7. leave other values to default\n",
    "- play with min_child_weight: increase it if train/val gap is large\n",
    "- Then tune max_depth or number_of_leave if LB score is way below CV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "\n",
    "don't start too early in competition\n",
    "\n",
    "a great model is better than an ensemble of weak models\n",
    "\n",
    "use same fold for all model\n",
    "\n",
    "use out of fold predictions as feature for second level of models (stacking)\n",
    "- Gap between CV and LB increase as there is some overfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
